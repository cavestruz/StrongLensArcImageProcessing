\documentclass{emulateapj}
\usepackage{amsmath} 
\usepackage{apjfonts} 
\usepackage{amssymb} 
\usepackage{xspace}
\usepackage{hyperref} 
\usepackage{natbib} 
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{epsf}
\usepackage{subfigure}
\bibliographystyle{apj_ads}
\usepackage[usenames,dvipsnames]{xcolor}
\newcommand{\comment}[1]{{\color{red} #1}}
\newcommand{\change}[1]{{\it\color{purple} #1}}
\newcommand{\todo}[1]{{\bf\color{blue} #1}}
%%%%%%%%%% User defined symbol %%%%%%
\def\gsim{\gtrsim}
\def\lsim{\lesssim}    
\def\Msun{M_\odot}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document} 
\shorttitle{MachLensFinder}
\shortauthors{Avestruz, Li, Lightman}
\submitted{The Astrophysical Journal} 
\slugcomment{The Astrophysical Journal, submitted}

\title{MachLensFinder - I: An automated Strong Lensing
  Identification Pipeline}

\author{Camille Avestruz\altaffilmark{1-3,$\dagger$}\thanks{E-mail:
    avestruz@uchicago.edu}, Nan Li\altaffilmark{2-4}, and Matthew Lightman\altaffilmark{5} }

\affil{
$^1${Enrico Fermi Institute, The University of Chicago, Chicago, IL 60637 U.S.A.}\\
$^2${Kavli Institute for Cosmological Physics, The University of Chicago, Chicago, IL 60637 U.S.A.}\\
$^3${Department of Astronomy \& Astrophysics, The University of Chicago, Chicago, IL 60637 U.S.A.};\\
$^4${High Energy Physics Division, Argonne National Laboratory,
Lemont, IL 60439};\\
$^5${Digital Intelligence, JPMorgan Chase, Chicago, IL 60603 U.S.A.};\\
$^\dagger${Provost's Postdoctoral Scholar at the University of Chicago}\\
  \href{mailto:avestruz@uchicago.edu}{avestruz@uchicago.edu}\\
}

\keywords{gravitational lensing --- methods : numerical --- methods: data analysis --- methods: statistical --- galaxies: elliptical --- surveys  }
  
%-------------------------------------------------%
\begin{abstract} 
Gravitational lensing offers a direct probe of the underlying mass
distribution of lensing systems, a window to the high redshift
universe, and a geometric probe of cosmological models.  The advent of
large scale surveys such as the Large Synoptic Sky Telescope and
Euclid has prompted a need for automatic and efficient identification
of strong lensing systems.  We present {\em MachLensFinder}, a
strong lensing identification pipeline that will be publicly released
as open source software.  In this first application, we employ a fast
feature extraction method, Histogram of Oriented Gradients (HOG), to
capture edge patterns that are characteristic of strong gravitational
arcs in galaxy-galaxy lensing.  We use logistic regression to train a
supervised classifier model on the HOG of \change{HST- and LSST}-like
images.  Our best performing models reach a completeness rate of
\change{XX\%} and purity rate of \change{XX\%} for an HST-like sample.
However, for one orbit of LSST, our model cannot distinguish between
strong lensing systems and non-strong lensing systems.  In absence of
the brightest cluster galaxy in LSST-like data, our trained model
reaches a completeness of \change{XX\%} and purity of \change{XX\%}.
Our results show one the most efficient and effective methods for
automated strong lensing identification in space-based surveys.  The
results also emphasize the need for further development and testing of
additional tools to maximize use of ground-based surveys.
\end{abstract}
%-------------------------------------------------%

%-------------------------------------------------%
\section{Introduction}
%-------------------------------------------------%

% Gravitational lensing as a geometric (?) test of cosmology.  Also
% probes underlying matter distribution in galaxy clusters, sensitive
% to things like recent accretion and environment.
Gravitational lensing occurs when intermediate fluctuations in the
matter density field deflect light from background sources
\citep[see][for a review]{kneibandnatarajan_11}.  In particular, the
deep gravitational potential wells of galaxy clusters provide a strong
lens, with lensed images that can be seen by eye.  Strong
gravitational lensing can manifest as visible giant arcs magnifying
high redshift galaxies \citep{lyndsandpetrosian_86,gladders_etal03},
multiply imaged quasars \citep{walsh_etal79}, and arclets.  Lensing
signatures probe the underlying dark matter distribution of galaxy
clusters \citep{warrenanddye_03}, probe high redshift galaxy formation
\citep{allam_etal07}, and provide a geometric test of cosmology via
comparison of predicted arc abundances and observed abundances
\citep{kochanek_96,chae_03,linder_04}.

% Arc identification can be done visually (e.g. that zooniverse
% thing).  But, in the advent of optical surveys, DES and LSST, we
% will be limited by our ability to automatically process the large
% amounts of data.  Wide field surveys - SPT in millimeter, Herschel
% in submillimeter; high resolution spectroscopic imaging with ALMA;
% optical surveys DES, HSC, LSST, and Euclid (Oguri \& Marshall 2010).
Visual arc identification has been effective through the use of
citizen science platforms (e.g. \citet{marshall_etal16,more_etal16}).
However, in the advent of large surveys, such as the Dark Energy
Survey (DES) and the Large Synoptic Sky Telescope (LSST), we will be
limited by our ability to automatically process large amounts of data
and select likely strong lensing clusters for follow up visual and
spectroscopic confirmation.  The strong lensing sector of
cluster-based cosmology requires an exploration of techniques in image
processing and classification.

% Image identification works: visual, ``robot'' identification -
% ArcFinder, RingFinder..., and citizen science (space warps).  Many
% of these rely on some explicit aspect of the image's morphology
% (give example), filtering, ... ....  Specifics of the methodology
% how the instrument affects the strong lensing signal, and the
% objects of interest.
Over the last decade, infrastructure has emerged for both large scale
visual and automated image classification. By nature, the human eye is
one of the best discriminators for image classification.  {\em
  SpaceWarps} is an example of citizen science based image
classification of strong lensing systems in Canada France Hawaii
Telescope Science (CFHTLS) telescope observations
\citep{marshall_etal16,more_etal16}.  Visual classification has been
quite successful for a dataset like CFHTLS, classifying \change{XX in
  XX time}.  However, future datasets like Euclid and LSST will be
\change{XX} times this size, challenging the scalability of a citizen
science approach.  The next step is to find automated methods with
performance comparable to or better than humans. {\em SpaceWarps} is a
part of the Zooniverse Project (CITE???), which also includes {\em
  Galaxy Zoo}, the citizen science based image classification of
galaxy types \citep{lintott_etal08}.  Galaxy classification is a
successful example of in which machine learning algorithms
successfully train models to classify images with comparable
performance to humans \citep{dieleman_etal15}.

% Describe general robot identification schemes that have been
% implemented
General automated, or ``robot'', identification refers to a classifier
where images are fed into an image processing pipeline that enhances
and extracts characteristic features as parameters on which a pattern
recognition piece is applied.  Examples of this include ArcFinder for
group or cluster scale lens \citep{seidelandbartelmann_07}, Ringfinder
for multiply imaged quasars\citep{gavazzi_etal14}, .  Each pipeline
.... ... and relies on the image morphology where ... ...  Note, that
the method best applied to ... ... and the objects of interest.

% Describe how machine learning is a step up
With ``machine learning'' algorithms, we can train a model to separate
a dataset according to some desired classification scheme.  However,
we still need to reduce the dimensionality of the problem by removing
information in the images that do not pertain to the classifier.
Examples of automated classification will...

% Previous works and our goal to make a universal pipeline 
Machine learning to automate.  Previous works (StrongML),
More+... Here, we focus on the improvement from preprocessing the
images and describe the pipeline, which is publicly available.
Depending on the image type, different preprocessing is likely
necessary (rings, multiply lensed quasars, etc.) Our goal is to create
an open source pipeline that is general enough for the user to select
or add appropriate image processing techniques to augment the features
of the lensed image, train and test data with different machine
learning techniques, and to select the sequence of methods that
maximizes completeness with optimal purity.

% This is the first in MachLensFinder series with initial pipeline
% results
This is the first in a series of {\em MachLensFinder} papers.  We
present our image processing and machine learning classifier pipeline.
Our image processing procedure parameterizes edges in our image in a
1-d histogram, enabling fast feature extraction that is similar to
techniques used to identify humans in security software systems.  We
train a model with logistic regression to identify features
corresponding to strong lensing systems.

We show results of the pipeline on mock galaxy-galaxy lens systems
observed by HST and LSST as respective examples of classifier
performance on optical space- and ground-based observations.
\citet{miraldaescudeandlehar_92} first suggested that massive
ellipticals would be likely frequent strong lensing sources in optical
surveys.  We train and test our pipeline on subsamples from 10,000
mock observed brightest cluster galaxy (BCG) strong lensing systems
and 10,000 BCG non-strong lensing systems.

% Arc finding must optimize for completeness and purity.  
In general, lensed image finding must optimize for completeness and
purity.  In the application of arc abundance comparisons with
cosmological predictions, completeness is important; we must be able
to understand the reasons and frequency of false negatives.  Since
spectroscopic confirmation of background lensed images is expensive,
purity is key; we must minimize false positives that would waste
follow up telescope time.  We discuss the performance of our image
processing and classification pipeline with respect to the
completeness and purity rates of our trained models and illustrate the
scalability of training our models [wc???].

Our paper is organized as follows. In Section~\ref{sec:methods} we
briefly describe the methods to generate the mock HST and LSST data
and our overall image processing and classification pipeline. We
present our results in Section~\ref{sec:results}, and our summary and
discussions in Section~\ref{sec:conclusions}.

%-------------------------------------------------%
\section{Methodology}
\label{sec:methods}
%-------------------------------------------------%
\subsection{Creating Simulated Images for Training Set and Testing Set}

We use mock Hubble Space Telescope (HST) and Large Synoptic Sky
Telescope (LSST) images generated with PICS (Pipeline for Images of
Cosmological Strong lensing) (Li et al. 2016) and LensPOP (Thomas
Colletti 2013) as training set and testing set. In these images, the
lensed images include the images of lens galaxies, lensed images, and
galaxies on the line of sight; the unlensed images include the images
of lens galaxies and the galaxies on the line of sight.

To produce simulated lensed galaxies, we first set mass models of the
lenses to be the Singular Isothermal Ellipsoid (SIE):

\begin{equation}
\kappa = \frac{\theta_{\rm E}}{2}\frac{1}{\sqrt{x_1^2/q+x_2^2 q}}.
\end{equation}


Where, $\theta_{\rm E}$ is the Einstein Radius, $q$ is axis
ratio. Einstein radius can be calculated according to the redshift of
the lens, the redshift of the source, and the velocity dispersion of
the lens galaxy.
\begin{equation}
\theta_{\rm E} = 4\pi\left(\frac{\sigma_v}{c}\right)\frac{D_{ls}}{D_{s}}, 
\end{equation}
where, $c$ is the speed of light, $\sigma_v$ is the velocity
dispersion of the lens galaxy, $D_{ls}$ and $D_{s}$ are the angular
diameter distance from the source plane to the lens plane and from the
source plane to the observer separately.

To rotate the lenses with a random orientation angles, we adopt the
transformation below:

\begin{eqnarray}
\label{eq:rotation}
\begin{bmatrix}

    x_{1}\prime      \\
    x_{2}\prime 
\end{bmatrix}
= 
\begin{bmatrix}
    \cos \phi  &  -\sin \phi      \\
    \sin \phi  &  ~\cos \phi      
\end{bmatrix} 
\begin{bmatrix}
    x_1      \\
    x_2      
\end{bmatrix} .
\end{eqnarray}

$\phi$ is the orientation angle.  According to above equations, we
need $\sigma_v$, $q$, $\phi$, redshift of the lens $z_l$, and redshift
of the source $z_s$ to derivative the lensing maps completely.
Velocity dispersion, ellipticity, and orientation are randomly chosen
from typical ranges of a galaxy, where $\sigma_v = [200, 320] km/s$,
$q = [0.5, 1.0]$, and $\phi = [0, 360]$. The redshift of the lens is
fixed at $z_l = 0.2$, and the redshift of the source is fixed at $z_s
= 1.0$. The images of source galaxies are extracted from the CANDELS
survey. We selected 33 bright galaxies, and relocated them to
projected positions, which are close to the caustic of the lensing
system, and the projected position of the lens is fixed in the center
of the field of view of each image. Once we have the lens, source, and
the parameters of the image simulation, i.e., $300 \times 300 {\rm
  pixels}^2$ and 0.03 arcsec per pixel per side, settled, the lensed
images can be produced by performing ray-tracing simulations.

The light distributions of the lens galaxies  are modeled as the elliptical Sersic profile,

\begin{equation}
I(R) = I_{\rm eff}~{\rm exp} \left\lbrace -b_{n} \left[ \left(
  \frac{R}{R_{\rm eff}}\right)^{1/n} - 1 \right ] \right\rbrace
\end{equation}


Where, $R = \sqrt[]{x_1^2 /q+x_2^2 q }$, $R_{\rm eff}$ is the
effective radius in units of arcsecond, $I_{\rm eff}$ is the intensity
at the effective radius, $n$ is the index of the Sersic profile, $q$
is the ellipticity, and We perform the similar transformation as
Eq. \ref{eq:rotation} to orient the sources.  We assume that the
distribution of light is following that of mass, so ellipticity and
orientation are the same as the SIE model. Matching the velocity
dispersion with the COSMOS morphological
catalog \footnote{\url{https://arxiv.org/pdf/1501.04977v2.pdf}}, we
assign effective radius, effective luminosity, and index to the light
profile.  We also assume the light center is on top of the mass
center, the noiseless images of lenses can be drawn. The galaxies on
the line of sight are constructed by cutting light-cones from the
Hubble Ultra Deep Field.  Stacking all these images together and
collaborating the magnitude of each component, we create primarily
simulated lensed images.

The final part is mock observing. To produce HST-like images, we do
not need to do anything further than stacking and magnitude
calibrating, because noise and PSF are involved when we are extracting
galaxies on the line of sight. To produce LSST-like images, we perform
re-binning, convolving with PSF, and adding noise serially; this
process is implemented in LensPop. We created single-epoch-LSST-like
images and stacked -LSST-like images for investigating the performance
and usability of our pipeline in LSST data. Figure 2.1 illustrates a
sample mock lensed and unlensed images from each telescope. From left
to right, the top panel images respectively correspond to a mock HST
lensing and non-lensing system. The mock HST images are 300 × 300 in
size, and the pixel size is 0.03 arcsecond. For the HST-like data-set,
many giant arcs are visually obvious. The middle panel images
respectively correspond to a mock single-epoch LSST lensing and
non-lensing systems. The bottom panel images respectively correspond
to a mock stacked LSST lensing and non-lensing systems. These images
are 45 × 45; the pixel size is 0.2 arcsecond. The ground based noise,
PSF, and limited resolution of the LSST-like data-set make visual
giant arc identification difficult. But when in the stacked LSST
images, the quality is better, because more data improves the signal
to noise ratio better.


In the training sets, we also generate unlensed images, the procedure
is similar to mock lensed images but getting rid of the part of
ray-tracing. 10000 lensed images and 10000 unlensed images are merged
to construct one training set. Furthermore, for investigating the
influence of BCG galaxies on the performance of our lens
identification pipeline in LSST data, we build another training sets
of the images without BCG galaxies.

%-------------------------------------------------%
\subsection{Mock Images}
%-------------------------------------------------%

% Summary of HST and LSST images
We use mock {\em Hubble Space Telescope} (HST) and {\em Large Synoptic
  Sky Telescope} (LSST) images generated with PICS (Pipeline for
Images of Cosmological Strong lensing) \citep{li_etal16}.  These
images include cluster-member galaxies and foreground stars.  Half of
the images used are strong lensing systems.  Here, lensed galaxy
images are ray-traced images of actual galaxies extracted from deep
Hubble Space Telescope observations.  The final mock images are
``observed'' with a realistic point spread function corresponding to
modeled detector artifacts for bright stars.  See \cite{li_etal16} for
more details.

Figure ~\ref{fig:mockimages} illustrates a sample mock lensed and
unlensed images from each telescope.  From left to right, the top
panel images respectively correspond to a mock HST lensing and
non-lensing system. The mock HST images are $n_\text{pix}\times
n_\text{pix}=300\times300$ in size.  For the HST-like dataset, many
giant arcs are visually obvious.

The bottom panel images respectively correspond to a mock LSST lensing
and non-lensing system.  These images are $n_\text{pix}\times
n_\text{pix}=45\times45$.  The ground based noise and limited
resolution of the LSST-like dataset make visual giant arc
identification difficult.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\begin{figure*}[t]\label{fig:mockimages}
\begin{center}
\includegraphics[width=1\columnwidth]{figures/105_lensed_imgs_1_color.pdf}\hspace{-20pt}
\includegraphics[width=1\columnwidth]{figures/105_unlensed_imgs_1_color.pdf}\\
\includegraphics[width=1\columnwidth]{figures/lsst_9_lensed_imgs_1_color.pdf}\hspace{-20pt}
\includegraphics[width=1\columnwidth]{figures/lsst_9_unlensed_imgs_1_color.pdf}
\caption{Top panel: Example mock HST images, with $n_\text{pix}\times
  n_\text{pix}=300\times300$.  The left corresponds to a strong
  gravitational lensing system with a visibly obvious giant arc.  The
  right corresponds to a non-lensing system.  Bottom panel: Example
  mock LSST images, with $n_\text{pix}\times n_\text{pix}=45\times45$.
  The resolution and noise of a ground based telescope is noticeably
  worse.  The left corresponds to a strong gravitational lensing
  system, and the right to a non-lensing system.  However, visual
  identification of giant arcs in the LSST images is very difficult.}
\end{center}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            

%-------------------------------------------------%
\subsection{Strong Arc Lensing Identification Pipeline}
%-------------------------------------------------%

To perform our analysis, we have used a variety of tools from {\em
  Scikit-learn} \citep{pedregosa_etal12}.  We outline the
identification pipeline in Figure~\ref{fig:pipeline}.  The first step
of our pipeline consists of a feature extraction stage, where our
feature vector is a {\em histogram of oriented gradients}
\citep{dalalandtriggs_05}.  We describe the method in
Section~\ref{sec:hog}.  We then use a machine learning algorithm to
train a classifier model on a subset of our images.  We have explored
both {\em Logistic Regression} (LR) and {\em Support Vector Machine}
(SVM) machine learning algorithms.  We test our trained model on an
independent subset of the images to assess the model performance.

Both the feature extrator, HOG, and the classifier contain a range of
parameters, which must be tested and optimized for peak model
performance.  We use {\em GridSearchCV} from {\em Scikit-learn} to
select appropriate parameters, and discuss these results in
Section~\ref{sec:gridsearch}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\begin{figure}[t]\label{fig:pipeline}
\begin{center}
\includegraphics[scale=0.55]{figures/supervised_classification_workflow.png}
\caption{** Cartoon of pipeline ** Mention that median filtering generates
visual artifacts that confuse the model. http://www.nltk.org/book/ch06.html}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            

%-------------------------------------------------%
\subsection{Feature Extractor: Histogram Oriented Gradients}\label{sec:hog}
%-------------------------------------------------%
Histogram oriented gradients (HOG) is a feature extraction method that
computes centered horizontal and vertical gradients with no smoothing.
Originally created for human detection, .... This computes the
gradient orientation and magnitudes in an image by first dividing the
image into $n\times n$ blocks of X\% overlap.  The gradient
orientation is then quantized into m bins. ``(optional) global image
normalisation computing the gradient image in x and y computing
gradient histograms normalising across blocks flattening into a
feature vector''\citep{dalalandtriggs_05}.

% Need to look into edge orientation histograms, scale-invariant
% feature transform descriptors, shape contexts.


% NEED TO FEED THE DESCRIPTORS TO SOMETHING WITH SUPERVISED LEARNING.
% (e.g. SVM, Neural network).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\begin{figure*}[t]\label{fig:hogvisual}
\begin{center}
\includegraphics[width=1\columnwidth]{figures/105_lensed_imgs_1or5ppc32cpb1.pdf}\hspace{-20pt}
\includegraphics[width=1\columnwidth]{figures/105_lensed_imgs_1or9ppc16cpb1.pdf}
\caption{Visualized HOG feature vectors of the sample lensed HST-like
  image from Figure~\ref{fig:mockimages}.  Left: The corresponding HOG
  visualization with N$_\text{orient}=5$, (32, 32) pixels per cell,
  and (1, 1) cells per block.  Right: The HOG visualization of the
  same mock image with N$_\text{orient}=9$, (16, 16) pixels per cell,
  and (1, 1) cells per block.  The brightest lines align with
  prominent edges.  Note, while fewer pixels per cell will better
  resolve the arc feature, the ``edge'' corresponding to the arc is
  less prominent.  }
\end{center}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            


%-------------------------------------------------%
\subsection{Machine Learning Algorithm: Logistic Regression}\label{sec:logreg}
%-------------------------------------------------%

Logisitic regression is a linear classifier, which finds the optimal
hyperplane between two classes of data points.  This hyperplane
defines the optimal model.  We must regularize the model to prevent
overfitting.  

In regularization, the goal is to minimize a cost function with
respect to ....  Larger values of $C_\text{LogReg}$ correspond to
increasing model complexity.  Figure~\ref{fig:regularization} shows
the accuracy of a model for selected HOG parameters as a function of
the regularization parameter, $C_\text{LogReg}$.  We plot the
regularization dependency for HOG parameterization of
$N_\text{orient}=9$, $PPC=(16, 16)$, $CPB=(1,1)$.  

The accuracy of a model can be defined as either the score of the
model on a test set, or the AUC for the ROC curve.


% Regularization coefficient:
% http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex5/ex5.html

%-------------------------------------------------%
\subsection{Machine Learning Algorithm: Support Vector Machine}
%-------------------------------------------------%


%-------------------------------------------------%
%\subsection{Deep Learning Algorithm}
%-------------------------------------------------%

%-------------------------------------------------%
\section{Results}
\label{sec:results}
%-------------------------------------------------%
%-------------------------------------------------%
\subsection{Optimized pipeline parameters}\label{sec:gridsearch}
%-------------------------------------------------%

\begin{table}
\caption{Grid Search of Pipeline Parameters}
\begin{center}
\begin{tabular}{cccccc}
\hline \\ [-0.2cm]
N$_{orient}$ & Pixels/Cell & Cells/Block & C$_{reg}$ & HOG size & Score \\ [0.2cm]
\hline \\ [-0.2cm]
\multicolumn{6}{c}{(a) HST-like data} \\ [0.2cm]
\hline \\ [-0.2cm]
9 & (4, 4) & (3, 3) & 50. & &$0.76181\pm0.00491$\\ [0.2cm]
9 & (4, 4) & (1, 1) & 50. & &$0.83403\pm0.00547$\\ [0.2cm]
9 & (8, 8) & (3, 3) & 50. & &$0.85417\pm0.00851$\\ [0.2cm] 
9 & (8, 8) & (1, 1) & 50. & &$0.86389\pm0.00687$\\ [0.2cm] 
9 & (16, 16) & (1, 1) & 50. & & $0.88542\pm0.00680$ \\ [0.2cm]
9 & (16, 16) & (3, 3) & 10. & &$0.89306\pm0.00687$ \\ [0.2cm]
8 & (16, 16) & (3, 3) & 50. & &$0.90000\pm0.00170$ \\ [0.2cm]
9 & (16, 16) & (3, 3) & 50. & &$0.90139\pm0.00804$ \\ [0.2cm]
\hline \\ [-0.2cm]
\multicolumn{6}{c}{(b) LSST-like data} \\ [0.2cm]
\hline \\ [-0.2cm]
& & & & &\\
& & & & &\\ [0.2cm]
\hline
\end{tabular}
\end{center}
\label{tab:gridsearch}
\tablecomments{Panel (a) shows the results of a grid search across a
  range of HOG parameters and regression coefficient with training and
  test sets of size 1440 with the HST-like mock dataset.  Panel (b)
  shows the corresponding results for training and test sets of size
  5760 with the LSST mock dataset.  We explore different HOG
  parameters in each dataset due to resolution and image size
  differences.}
\end{table}

We run a grid search across parameters that should reasonably sample
the arc edges in either the HST- or LSST-like mock observations.
Recall, the HST-like images are $300\times300$ pixels per image, while
the LSST-like mock observations are $45\times45$ pixels per image.  In
general, must run a grid search in order to best optimize the pipeline
parameters.

We must first estimate the size of a cell that will contain a coherent
arc feature.  To first order approximation, subdivisions of cells that
are 1/100th the area of the entire image should contain coherent arc
edges that span an elongated shape within arc-containing cells.
Therefore, we perturb the pixels per cell parameter about (30, 30) for
the HST-like images, and (4, 4) for the LSST-like images.

Next, the cells per block parameter determines the normalization of
each cell with respect to the neighboring cell.  In general, this will
downweight arc-like edges in cells that neighbor very bright cells,
such as cells that cover the BCG.  We therefore vary the cells per
block parameter between (1, 1) and (3, 3).

The number of orientations will determine the sampling of rounded
edges.  For example, if we only have two orientations, an arc-like
feature in a cell will appear as .... However, contributions from a
cluster or line-of-sight galaxy in the same cell will ... the
normalizations in the histogram.  The resolution of the overall image
will also limit the additional information that an increase in
$N_\text{orient}$ will provide.  For the LSST-like data,
$N_\text{orient}=9$ compared with $N_\text{orient}=5$ only
results in a $???\%$ improvement.  In the case of the HST-like data,
$N_\text{orient}=9$ compared with $N_\text{orient}=5$
results in a $???\%$ improvement.

Note, we also include the size of the extracted features, e.g. the
length of the HOG feature vector.  A longer feature vector will
increase the amount of time required to train the model.
Additionally, for fixed memory restrictions, there is a tradeoff
between the length of the feature vector and the size of the training
set.  The logistic regression coefficient will also increase the
training time, \todo{since larger coefficients correspond to a more
  singular matrix}.  We discuss the score dependence on logistic
regression coefficient in Section\ref{sec:unregularizedfit}.

%-------------------------------------------------%
\subsection{Receiver Operatic Characteristic}
%-------------------------------------------------%

In this section, we discuss the Receiver Operating Characteristic
(ROC) curve, which shows the true positive rate as a function of false
positive rate for a given model and test set.  The true positive rate
is defined as the number of true positives divided by the total number
of positives (completeness of the positive identifications).  The
false positive rate is defined as the number of false positives
divided by the total number of negatives (impurity of the
identifications).  The ROC curve illustrates the performance of our
trained model as we vary the discrimination threshold.

The classifier model assigns a score to each test image, which is a
probability that the image is a strong lensing system.  To construct
the ROC curve, the test images are ranked by probability and the
discrimination threshold varied.  

For very high discrimination threshold, we have maximal purity, but a
low completeness (bottom left region of the ROC).  FOr a very low
discrimination threshold, we have maximal completeness, but minimal
purity (top right region of the ROC).  The ideal model would have a
data point on the ROC curve that sits on the top left corner,
i.e. $(x, y) = (0, 1)$.

In the context of strong lensing systems, we wish to maximize the true
positive rate so we have a representative count of the fraction of
strong lensing systems in an observed volume of the universe.  We also
want to minimize the false positive rate.  Positively identified
strong lensing systems will require expensive spectroscopic follow-up
for validation.  The steepness of the ROC curve indicates how well the
model will optimize the two, and we can characterize the steepness by
the area under the curve (AUC).  A perfect model would have an AUC of
1, with data points along $(x=0, 0\leq y\leq1)$, and $(0\leq x\leq 1,
y=1)$.

% HST model performance
Figure~\ref{fig:ROC} shows the ROC curves for three different HOG
parameterizations for the HST-like dataset in red, green, and blue.
The parameters and corresponding AUC for each curve are labeled in the
legend.  The models for the HST-like dataset perform well, and have
AUCs very close to 1.  

% LSST model performance
On the other hand, the model for our LSST-like dataset (solid magenta)
does not perform well.  The dashed diagonal like corresponds to a
model that is as good as a random classifier, e.g. a coin-flip, for
each test image.  The LSST-like ROC curve does marginally better, and
the $AUC=0.527$ is within the statistical noise of the sample.  

% No BCG LSST
One possible method of improving the detection of strong lensing
systems is to model out the BCG.  The dotted magenta ROC curve
corresponds to a model trained and tested on LSST-like data where the
BCG has not been added to the mock image.  This model results in an
AUC of 0.64, performing better than random.  In other words, if we
were to perfectly model out the BCG contribution in LSST-like data, we
could improve the performance by $\sim10\%$.

% Choosing C
The ROCs correspond to models that have been trained with logistic
regression regularization parameter, $C_\text{LogReg}$, that best
describes the training dataset without overfitting.  We discuss the
choice of regularization parameter in
Section~\ref{sec:regularizedfit}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\begin{figure}[t]\label{fig:ROCHST}
\begin{center}
\includegraphics[width=\columnwidth]{figures/ROC_HST_HOG.png}
\caption{Red, green, blue: ROC curves for a models trained on the
  HST-like data with the Logistic Regression algorithm.  The magenta
  solid (dotted) curve show the ROC curve for the LSST-like data with
  (without) the BCG.  The corresponding parameters are labeled: N for
  number of orientations, ppc for pixels-per-cell, cpb for
  cells-per-block, and C for best regularization parameter.  The AUC
  for each model is also labeled in the legend, where an AUC close to
  1 is a near-ideal model, and an AUC close to 0.5 is a model that
  performs as well as a coin-flip.}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\begin{figure}[t]\label{fig:ROCLSST}
\begin{center}
\includegraphics[width=\columnwidth]{figures/ROC_LSST_HOG.png}
\caption{Red, green, blue: ROC curves for a models trained on the
  HST-like data with the Logistic Regression algorithm.  The magenta
  solid (dotted) curve show the ROC curve for the LSST-like data with
  (without) the BCG.  The corresponding parameters are labeled: N for
  number of orientations, ppc for pixels-per-cell, cpb for
  cells-per-block, and C for best regularization parameter.  The AUC
  for each model is also labeled in the legend, where an AUC close to
  1 is a near-ideal model, and an AUC close to 0.5 is a model that
  performs as well as a coin-flip.}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            

%-------------------------------------------------%
\subsection{Effects of Regularization on Model Performance}\label{sec:regularizedfit}
%-------------------------------------------------% 

% Logistic regression and regularization
As described in Section~\ref{sec:logreg}, logistic regression trains a
model with complexity determined by the regularization parameter,
$C_\text{LogReg}$.  Increasing model complexity (larger
$C_\text{LogReg}$), will better describe features in the training
set[wc?].  However, an overly complex model will overfit the training
set at the expense of its performance on any independent test set.
The regularization parameter ultimately defines the model performance,
and we must perform a parameter search to identify the optimal value
for $C_\text{LogReg}$.

%  Our running over C - score and AUC as characterizations
Figure~\ref{fig:regularization} shows the model performance as a
function of regularization parameter.  The left panel shows the
accuracy, or score, of the model as a function of regularization
parameter.  The accuracy is simply the percentage of correct
predictions, true positives and true negatives, out of a given set.
The right panel shows the AUC of the model as a function of
regularization parameter.  The solid and dotted lines respectively
correspond to the model performance on the test and training set.

% Training behavior with C
For either characterization of the model, the model performance on the
training set increases and asymptotes with $C_\text{LogReg}$.  With
increasing model complexity, the model better fits the data set it has
been trained on.  [wc???] This is analogous to fitting a seventh order
polynomial to seven data points, where the fitting function will go
through every point but will not likely predict additional points.

% Test behavior with C
We use the model performance on a test set to determine the best value
for $C_\text{LogReg}$ that will not overfit the training data.  With
increasing model complexity, we are able to better able to capture
features that are generally characteristic of strong lensing systems
with arcs.  However, past a certain $C_\text{LogReg}$, the model
performance on the test set decreases, as it has overfit the training
set.

% Scores vs. regularization on larger train/test set with comparable
% grid search in parameterization
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\begin{figure*}[t]\label{fig:regularization}
\begin{center}
\includegraphics[width=\columnwidth]{figures/regularized_accuracy.pdf}\hspace{-20pt}
\includegraphics[width=1\columnwidth]{figures/regularized_AUC.pdf}
\caption{Left: Accuracy of the model with varying regularization.
  Each color corresponds to a model trained for features extracted
  with a different set of HOG parameters.  The dotted (solid) lines
  show the performance of the model on the training (test) set.
  Right: AUC of the model with varying regularization.  Linestyles and
  colors are similar to those in the left panel.  AUC is a better
  characterization of model performance, since the accuracy is
  determined for fixed classification threshold [wc?], but peak model
  performance can occur at a different classification threshold.}
\end{center}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            


%-------------------------------------------------%
\subsection{Failed identifications}\label{sec:failedids}
%-------------------------------------------------%

The best ROC curve, HST:N5ppc32cpb4C100, illustrates that we can lower
the classification threshold and improve the true positive rate by
$\sim10\%$ and sacrifice an increase in false positive rate by
$\lsim5\%$.  In this section, we examine the ``borderline'' images,
which transition from negative classification to positive calculation
in the steepest part of the AUC curve.  Borderline images correspond
to candidates for spectroscopic follow-up.  The model is less
confident about this subset of images, and these would therefore be
good candidates for follow-up human classification and/or
spectroscopic follow-up (CITE HERE???).

\todo{Figure ~\ref{fig:falseids} shows a random sample of 10 borderline
images.  Common features...????}


%-------------------------------------------------%
\subsection{Effects of BCG contamination}
%-------------------------------------------------%

Previous works to identify strong lensing systems have often modeled
out the BCG from images to better identify lensed images (???CITE
HERE???).  Here, we test the effects of the BCG by training and
testing models on mock data where the BCG has not been included.  By
construction, these results show the maximal improvement that can be
obtained from BCG subtraction.

Figure~\ref{fig:ROCnobcg} shows the \todo{ROC curve for models of HOG
  feature vectors of mock data with no BCG.  The feature vectors are
  the same HOG parameters as in Figure~\ref{fig:ROC} }.  \todo{The
  grid search over parameter space was similar to that for data with
  the BCG.}

???????

%-------------------------------------------------%
\section{Summary and Discussions}
\label{sec:conclusions}
%-------------------------------------------------%

We summarize key points below:

%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{itemize}

\item Publicly released pipeline will be... swap out feature
  extraction, machine learning algorithm, perform parameter searches,
  and train a model with best parameters.

\item HOG efficiently extracts features that well-describe strong
  lensing systems with an arc. Performance of the model... SVM doesn't
  make a difference.

\item Ground based... noise... Model out BCG. 

\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%

These results indicate that...

% Caveat of the mock data set
One major caveat ... training and test mock images... LOS galaxies all
from Hubble Ultra Deep field.  Success measured by performance on the
small sample of systems observed with HST.

% 

\acknowledgments CA acknowledges support from the Kavli Institute of
Cosmological Physics, the Enrico Fermi Institute at the University of
Chicago, and the University of Chicago Provost's Office.
\lastpagefootnotes


\bibliography{ms}

\end{document}  

