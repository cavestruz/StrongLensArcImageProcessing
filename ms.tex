\documentclass{emulateapj}
\usepackage{amsmath} 
\usepackage{apjfonts} 
\usepackage{amssymb} 
\usepackage{xspace}
\usepackage{hyperref} 
\usepackage{natbib} 
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{epsf}
\usepackage{subfigure}
\bibliographystyle{apj_ads}
\usepackage[usenames,dvipsnames]{xcolor}
\newcommand{\comment}[1]{{\color{red} #1}}
\newcommand{\wording}[1]{{\it\color{purple} #1}}
\newcommand{\todo}[1]{{\bf\color{blue} #1}}
%%%%%%%%%% User defined symbol %%%%%%
\def\gsim{\gtrsim}
\def\lsim{\lesssim}    
\def\Msun{M_\odot}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document} 
\shorttitle{Automated Lensing Learner}
\shortauthors{Avestruz, Li, Lightman}
\submitted{The Astrophysical Journal} 
\slugcomment{The Astrophysical Journal, submitted}

\title{Automated Lensing Learner - I: An automated Strong Lensing
  Identification Pipeline}

\author{Camille Avestruz\altaffilmark{1-3,$\dagger$}\thanks{E-mail:
    avestruz@uchicago.edu}, Nan Li\altaffilmark{2-4}, and Matthew Lightman\altaffilmark{5} }

\affil{
$^1${Enrico Fermi Institute, The University of Chicago, Chicago, IL 60637 U.S.A.}\\
$^2${Kavli Institute for Cosmological Physics, The University of Chicago, Chicago, IL 60637 U.S.A.}\\
$^3${Department of Astronomy \& Astrophysics, The University of Chicago, Chicago, IL 60637 U.S.A.};\\
$^4${High Energy Physics Division, Argonne National Laboratory,
Lemont, IL 60439};\\
$^5${Digital Intelligence, JPMorgan Chase, Chicago, IL 60603 U.S.A.};\\
$^\dagger${Provost's Postdoctoral Scholar at the University of Chicago}\\
  \href{mailto:avestruz@uchicago.edu}{avestruz@uchicago.edu}\\
}

\keywords{gravitational lensing --- methods : numerical --- methods: data analysis --- methods: statistical --- galaxies: elliptical --- surveys  }
  
%-------------------------------------------------%
\begin{abstract} 
Gravitational lensing offers a direct probe of the underlying mass
distribution of lensing systems, a window to the high redshift
universe, and a geometric probe of cosmological models.  The advent of
large scale surveys such as the Large Synoptic Sky Telescope and
Euclid has prompted a need for automatic and efficient identification
of strong lensing systems.  We present {\em (ALL) Automated Lensing
  Learner}, a strong lensing identification pipeline that will be
publicly released as open source software.  In this first application,
we employ a fast feature extraction method, Histogram of Oriented
Gradients (HOG), to capture edge patterns that are characteristic of
strong gravitational arcs in galaxy-galaxy lensing.  We use logistic
regression to train a supervised classifier model on the HOG of HST-
and LSST-like images.  We use an area under the curve (AUC) of a
Receiver Operating Characteristic curve to assess model performance;
$AUC=1.0$ is an ideal trained classifier, and $AUC=0.5$ does not
provide any classification information.  Our best performing models on
a training set of 10,000 lens containing images and 10,000 non-lens
containing images exhibit an AUC of 0.975 for an HST-like sample.
However, for one orbit of LSST, our model only reaches an AUC of
0.625.  For 10-year mock LSST observations, the AUC improves to 0.809.
Model performance appears to continually improve with the size of the
training set.  Models trained on fewer images perform better in
absence of the lens galaxy, which can decrease the false positive rate
by blending with light from a lensed galaxy image near the central
lensing galaxy.  However, with larger training data sets, information
from the lensing galaxy improves model performance.  \wording{Our
  results show efficient and effective methods for automated strong
  lensing identification with scalable linear methods that are easy to
  parallelize with existing open source tools and are reproducible on
  personal laptop computers.}
\end{abstract}
%-------------------------------------------------%

%-------------------------------------------------%
\section{Introduction}
%-------------------------------------------------%

% Gravitational lensing as a geometric (?) test of cosmology.  Also
% probes underlying matter distribution in galaxy clusters, sensitive
% to things like recent accretion and environment.
Gravitational lensing occurs when intermediate fluctuations in the
matter density field deflect light from background sources
\citep[see][for a review]{kneibandnatarajan_11}.  Strong gravitational
lensing can manifest as visible giant arcs magnifying high redshift
galaxies \citep{lyndsandpetrosian_86,gladders_etal03}, multiply imaged
quasars \citep{walsh_etal79}, and arclets.  Lensing signatures probe
the underlying dark matter distribution of the lens
\citep{warrenanddye_03}, high redshift galaxy formation
\citep{allam_etal07}, and provide a geometric test of cosmology via
comparison of predicted arc abundances and observed abundances
\citep{kochanek_96,chae_03,linder_04} and time-delay signals from
multiply imaged quasars ???\citep{}.

% Application of constraining mass profiles - large numbers needed.
The application of strong gravitational lensing to constrain the mass
distribution of \todo{early-type galaxies (ETGs)} necessitates large
samples of galaxy-galaxy lensing systems.  The strong lensing
signature... EDITING HERE These systems exhibit a background source
galaxy that the ETG deflects into a partial of full arc shaped
Einstein ring.

% Arc identification can be done visually (e.g. that zooniverse
% thing).  But, in the advent of optical surveys, DES and LSST, we
% will be limited by our ability to automatically process the large
% amounts of data.  Wide field surveys - SPT in millimeter, Herschel
% in submillimeter; high resolution spectroscopic imaging with ALMA;
% optical surveys DES, HSC, LSST, and Euclid (Oguri \& Marshall 2010).
Visual arc identification has been effective through the use of
citizen science platforms (e.g. \citet{marshall_etal16,more_etal16}).
However, in the advent of large surveys, such as the Dark Energy
Survey (DES) and the Large Synoptic Sky Telescope (LSST), we will be
limited by our ability to automatically process large amounts of data
and select likely strong lensing candidates for follow up visual and
spectroscopic confirmation.  The strong lensing sector requires an
exploration of techniques in image processing and classification.

% Image identification works: visual, ``robot'' identification -
% ArcFinder, RingFinder..., and citizen science (space warps).  Many
% of these rely on some explicit aspect of the image's morphology
% (give example), filtering, ... ....  Specifics of the methodology
% how the instrument affects the strong lensing signal, and the
% objects of interest.
Over the last decade, infrastructure for both large scale visual and
automated image classification emerged. By nature, the human eye is
one of the best discriminators for image classification.  {\em
  SpaceWarps} is an example of citizen science based image
classification of strong lensing systems in Canada France Hawaii
Telescope Science (CFHTLS) telescope observations
\citep{marshall_etal16,more_etal16}.  Visual classification has been
quite successful for a dataset like CFHTLS, classifying \change{XX in
  XX time}.  However, future datasets like Euclid and LSST will be
\change{XX} times this size, challenging the scalability of a citizen
science approach.  The next step is to find automated methods with
performance comparable to or better than humans. {\em SpaceWarps} is a
part of the Zooniverse Project (CITE???), which also includes {\em
  Galaxy Zoo}, the citizen science based image classification of
galaxy types \citep{lintott_etal08}.  Galaxy classification is a
successful example of in which machine learning algorithms
successfully train models to classify images with comparable
performance to humans \citep{dieleman_etal15}.

% Describe general robot identification schemes that have been
% implemented
General automated, or ``robot'', identification refers to a classifier
where images are fed into an image processing pipeline that enhances
and extracts characteristic features as parameters on which a pattern
recognition piece is applied.  Examples of this include ArcFinder for
group or cluster scale lens \citep{seidelandbartelmann_07}, Ringfinder
for multiply imaged quasars\citep{gavazzi_etal14}, and ???.  Each
pipeline .... ... and relies on the image morphology where ... ...
Note, that the method best applied to ... ... and the objects of
interest.

% Describe how machine learning is a step up
With ``machine learning'' algorithms, we can train a model to separate
a dataset according to some desired classification scheme.  However,
we still need to reduce the dimensionality of the problem by removing
information in the images that do not pertain to the classifier.
Examples of automated classification will...

% Previous works and our goal to make a universal pipeline 
Machine learning to automate.  Previous works (StrongML),
More+... Here, we focus on the improvement from preprocessing the
images and describe the pipeline, which is publicly available.
Depending on the image type, different preprocessing is likely
necessary (rings, multiply lensed quasars, etc.) Our goal is to create
an open source pipeline that is general enough for the user to select
or add appropriate image processing techniques to augment the features
of the lensed image, train and test data with different machine
learning techniques, and to select the sequence of methods that
maximizes completeness with optimal purity.

% This is the first in MachLensFinder series with initial pipeline
% results
This is the first in a series of {\em MachLensFinder} papers.  We
present our image processing and machine learning classifier pipeline.
Our image processing procedure parameterizes edges in our image in a
1-d histogram, enabling fast feature extraction that is similar to
techniques used to identify humans in security software systems.  We
train a model with logistic regression to identify features
corresponding to strong lensing systems.

% Summary of results
We show results of the pipeline on mock galaxy-galaxy lens systems
observed by HST and LSST as respective examples of classifier
performance on optical space- and ground-based observations.
\citet{miraldaescudeandlehar_92} first suggested that massive
ellipticals would be likely frequent strong lensing sources in optical
surveys.  We train and test our pipeline on subsamples from 10,000
mock observed brightest cluster galaxy (BCG) strong lensing systems
and 10,000 BCG non-strong lensing systems.

% Need to optimize for completeness and purity.  Arc counts and mass
% reconstruction.
In general, lensed image finding must optimize for completeness and
purity.  In the application of arc abundance comparisons with
cosmological predictions, completeness is important; we must be able
to understand the reasons and frequency of false negatives.  In the
application of mass reconstruction of the lensing object, completeness
in strong lensing system identifications will allow us to constrain
the mass-to-light ratio for a large number of objects.  

% Purity issues
Since spectroscopic confirmation of background lensed images is
expensive, purity is key; we must minimize false positives that would
waste follow up telescope time.  We discuss the performance of our
image processing and classification pipeline with respect to the
completeness and purity rates of our trained models and illustrate the
scalability of training our models [wc???].  

Our paper is organized as follows. In Section~\ref{sec:methods} we
briefly describe the methods to generate the mock HST and LSST data
and our overall image processing and classification pipeline. We
present our results in Section~\ref{sec:results}, and our summary and
discussions in Section~\ref{sec:conclusions}.

%-------------------------------------------------%
\section{Methodology}
\label{sec:methods}
%-------------------------------------------------%
\subsection{Creating Simulated Images for Training Set and Testing Set}

We use mock Hubble Space Telescope (HST) and Large Synoptic Sky
Telescope (LSST) images generated with PICS (Pipeline for Images of
Cosmological Strong lensing) (Li et al. 2016) and LensPOP (Thomas
Colletti 2013) as training set and testing set. In these images, the
lensed images include the images of lens galaxies, lensed images, and
galaxies on the line of sight; the unlensed images include the images
of lens galaxies and the galaxies on the line of sight.

To produce simulated lensed galaxies, we first set mass models of the
lenses to be the Singular Isothermal Ellipsoid (SIE):

\begin{equation}
\kappa = \frac{\theta_{\rm E}}{2}\frac{1}{\sqrt{x_1^2/q+x_2^2 q}}.
\end{equation}


Where, $\theta_{\rm E}$ is the Einstein Radius, $q$ is axis
ratio. Einstein radius can be calculated according to the redshift of
the lens, the redshift of the source, and the velocity dispersion of
the lens galaxy.
\begin{equation}
\theta_{\rm E} = 4\pi\left(\frac{\sigma_v}{c}\right)\frac{D_{ls}}{D_{s}}, 
\end{equation}
where, $c$ is the speed of light, $\sigma_v$ is the velocity
dispersion of the lens galaxy, $D_{ls}$ and $D_{s}$ are the angular
diameter distance from the source plane to the lens plane and from the
source plane to the observer separately.

To rotate the lenses with a random orientation angles, we adopt the
transformation below:

\begin{eqnarray}
\label{eq:rotation}
\begin{bmatrix}

    x_{1}\prime      \\
    x_{2}\prime 
\end{bmatrix}
= 
\begin{bmatrix}
    \cos \phi  &  -\sin \phi      \\
    \sin \phi  &  ~\cos \phi      
\end{bmatrix} 
\begin{bmatrix}
    x_1      \\
    x_2      
\end{bmatrix} .
\end{eqnarray}

$\phi$ is the orientation angle.  According to above equations, we
need $\sigma_v$, $q$, $\phi$, redshift of the lens $z_l$, and redshift
of the source $z_s$ to derivative the lensing maps completely.
Velocity dispersion, ellipticity, and orientation are randomly chosen
from typical ranges of a galaxy, where $\sigma_v = [200, 320] km/s$,
$q = [0.5, 1.0]$, and $\phi = [0, 360]$. The redshift of the lens is
fixed at $z_l = 0.2$, and the redshift of the source is fixed at $z_s
= 1.0$. The images of source galaxies are extracted from the CANDELS
survey. We selected 33 bright galaxies, and relocated them to
projected positions, which are close to the caustic of the lensing
system, and the projected position of the lens is fixed in the center
of the field of view of each image. Once we have the lens, source, and
the parameters of the image simulation, i.e., $300 \times 300 {\rm
  pixels}^2$ and 0.03 arcsec per pixel per side, settled, the lensed
images can be produced by performing ray-tracing simulations.

The light distributions of the lens galaxies  are modeled as the elliptical Sersic profile,

\begin{equation}
I(R) = I_{\rm eff}~{\rm exp} \left\lbrace -b_{n} \left[ \left(
  \frac{R}{R_{\rm eff}}\right)^{1/n} - 1 \right ] \right\rbrace
\end{equation}


Where, $R = \sqrt[]{x_1^2 /q+x_2^2 q }$, $R_{\rm eff}$ is the
effective radius in units of arcsecond, $I_{\rm eff}$ is the intensity
at the effective radius, $n$ is the index of the Sersic profile, $q$
is the ellipticity, and We perform the similar transformation as
Eq. \ref{eq:rotation} to orient the sources.  We assume that the
distribution of light is following that of mass, so ellipticity and
orientation are the same as the SIE model. Matching the velocity
dispersion with the COSMOS morphological
catalog \footnote{\url{https://arxiv.org/pdf/1501.04977v2.pdf}}, we
assign effective radius, effective luminosity, and index to the light
profile.  We also assume the light center is on top of the mass
center, the noiseless images of lenses can be drawn. The galaxies on
the line of sight are constructed by cutting light-cones from the
Hubble Ultra Deep Field.  Stacking all these images together and
collaborating the magnitude of each component, we create primarily
simulated lensed images.

The final part is mock observing. To produce HST-like images, we do
not need to do anything further than stacking and magnitude
calibrating, because noise and PSF are involved when we are extracting
galaxies on the line of sight. To produce LSST-like images, we perform
re-binning, convolving with PSF, and adding noise serially; this
process is implemented in LensPop. We created single-epoch-LSST-like
images and stacked -LSST-like images for investigating the performance
and usability of our pipeline in LSST data. Figure 2.1 illustrates a
sample mock lensed and unlensed images from each telescope. From left
to right, the top panel images respectively correspond to a mock HST
lensing and non-lensing system. The mock HST images are 300 × 300 in
size, and the pixel size is 0.03 arcsecond. For the HST-like data-set,
many giant arcs are visually obvious. The middle panel images
respectively correspond to a mock single-epoch LSST lensing and
non-lensing systems. The bottom panel images respectively correspond
to a mock stacked LSST lensing and non-lensing systems. These images
are 45 × 45; the pixel size is 0.2 arcsecond. The ground based noise,
PSF, and limited resolution of the LSST-like data-set make visual
giant arc identification difficult. But when in the stacked LSST
images, the quality is better, because more data improves the signal
to noise ratio better.


In the training sets, we also generate unlensed images, the procedure
is similar to mock lensed images but getting rid of the part of
ray-tracing. 10000 lensed images and 10000 unlensed images are merged
to construct one training set. Furthermore, for investigating the
influence of BCG galaxies on the performance of our lens
identification pipeline in LSST data, we build another training sets
of the images without BCG galaxies.

%-------------------------------------------------%
\subsection{Mock Images}
%-------------------------------------------------%

% Summary of HST and LSST images
We use mock {\em Hubble Space Telescope} (HST) and {\em Large Synoptic
  Sky Telescope} (LSST) images generated with PICS (Pipeline for
Images of Cosmological Strong lensing) \citep{li_etal16} and
LensPop\citep{collett_etal15}.  These images include cluster-member
galaxies and foreground stars.  Half of the images used are strong
lensing systems.  Here, lensed galaxy images are ray-traced images of
actual galaxies extracted from deep Hubble Space Telescope
observations.  The final mock images are ``observed'' with a realistic
point spread function corresponding to modeled detector artifacts for
bright stars.  See \cite{li_etal16} for more details.

Figure ~\ref{fig:mockimages} illustrates a sample mock lensed and
unlensed images from each telescope.  The top left and right panel
images respectively correspond to a mock HST lensing and non-lensing
system. The mock HST images are $n_\text{pix}\times
n_\text{pix}=300\times300$ in size.  For the HST-like dataset, many
giant arcs are visually obvious.

The middle row corresponds to the same simulated systems, as observed
with a single epoch of LSST.  These images are $n_\text{pix}\times
n_\text{pix}=45\times45$.  The ground based noise and limited
resolution of the LSST-like dataset make visual giant arc
identification difficult.  However, by stacking observations over the
course of the LSST survey, we recover the arc feature, albeit at a
much lower resolution than with the HST-like image. The bottom row
shows the mock stacked observation, where the lensed background galaxy
is now visible.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\begin{figure*}[t]\label{fig:mockimages}
\begin{center}
\includegraphics[width=2\columnwidth]{figures/tiles/compilations/telescope_compilation.pdf}
\caption{Left to right show example mock HST, LSST 10 year, and LSST 1
  year images.  The top row corresponds to a lensing system with a
  very visible arc signature, and the bottom row to a lensing system
  that is less obvious.  Example mock HST images have
  $n_\text{pix}\times n_\text{pix}=300\times300$.  Example mock LSST
  images have $n_\text{pix}\times n_\text{pix}=45\times45$.  The
  resolution and noise of a ground based telescope is noticeably
  worse. Visual identification of giant arcs in the LSST images in the
  bottom row is very difficult.}
\end{center}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            

%-------------------------------------------------%
\subsection{Strong Arc Lensing Identification Pipeline}
%-------------------------------------------------%

To perform our analysis, we have used tools from {\em Scikit-learn}
\citep{pedregosa_etal12}.  We outline the identification pipeline in
Figure~\ref{fig:pipeline}, which is a general description of
supervised classification.  Supervised classification is a class of
machine learning where the labels of classification in the training
set are known.  In our case, the labels are lens and non-lens
containing images.

The first step of our pipeline consists of a feature extraction stage,
where our feature vector is a {\em histogram of oriented gradients}
\citep{dalalandtriggs_05} that quantifies edges in the image.  We
describe the method and parameter search in Section~\ref{sec:hog}.  We
then use {\em Logistic Regression} (LR), a machine learning algorithm
described in Section~\ref{sec:LR}, to train a classifier model on a
subset of our images.  LR requires a parameter search over the
regression coefficient, $C_{LogReg}$, which we explore in
Section~\ref{sec:regularization}.  We briefly comment that tests with
\todo{{\em Support Vector Machine} (SVM) as an alternative machine
  learning algorithm yielded negligible performance improvement, and
  increased computation cost.  We do not include SVM in our final
  analysis and comparisons.}

Both the feature extrator, HOG, and the linear classifier, LR, contain
parameters, which must be tested and optimized for peak model
performance.  We use {\em GridSearchCV} from {\em Scikit-learn} to
select cross-validated parameters, and discuss this step of our
methodology in Section~\ref{sec:gridsearch}.

The second step of our analysis is to test our trained model on an
independent subset of the images to assess the model performance.
Here, we evaluate the model on each test image, predicting a
likelihood (``score'') between 0 and 1 that this is a lens containing
system.  This ``holdout set'' is not used in any of our parameter
searches to keep our test metric independent of tuning.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\begin{figure}[t]\label{fig:pipeline}
\begin{center}
\includegraphics[scale=0.55]{figures/supervised_classification_workflow.png}
\caption{Cartoon of pipeline \footnote{from
    http://www.nltk.org/book/ch06.html} for supervised classification.
  In our case, the labels are either lens or non-lens containing
  images.  Our input is the set of mock HST- or LSST-like
  observations.  The feature extractor is the histogram of oriented
  gradients, producing an N-dimensional feature vector that quantify
  edges in our images.  Our machine learning algorithm is logistic
  regression.}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            

%-------------------------------------------------%
\subsection{Feature Extractor: Histogram Oriented Gradients}\label{sec:hog}
%-------------------------------------------------%
% HOG intro - 
Originally created for human detection in computer vision, histogram
oriented gradients (HOG) is a feature extraction method that computes
centered horizontal and vertical gradients. HOG is relatively robust
to noise in the image, and is a fairly fast transform that describes
edges.  Details can be found in \citet{dalalandtriggs_05}, but we
describe the procedure here.  The end result of HOG is a one
dimensional histogram computed as follows.

% Division of the image  
HOG first divides the image into blocks of \wording{50\% overlap}.
Each block contains $m\times m$ cells-per-block that each contain
$n_{pix}\times n_{pix}$ pixels-per-cell.  The computed gradient
orientation is quantized into $N_{orient}$ bins.

% Initial binning
The orientation gradient of all pixels within each cell are binned
into the quantized orientations, providing a net gradient description
within that cell.  As an example, for $N_{orient}=3$, our bins are
centered at $\theta=0, 2\pi/3, 4\pi/3$ in radians.  If a cell has no
gradient in the vertical direction, and only a gradient across the
horizontal direction, say in the positive x-direction with
$\theta=\pi/2$, it will contribute 75\% of its magnitude to the
$\theta=2\pi/3$ bin, and 25\% of its magnitude to the $\theta=0$ bin.
The bins in all cells are then concatenated to make a larger feature
vector that is $N_{orient}$\times$N_{cells}$.

% Normalization
The last step is a normalization procedure to control for illumination
effects.  Here, the sub-histograms of each cell within the same block
are normalized with respect to one another before the transformation
returns the final feature vector.  The division of the image and the
quantization of orientations provide a total of 3 parameters in HOG:
$N_{orient}$, cells-per-block, and pixels-per-cell.  We discuss how we
select parameters using cross-validation in
Section~\ref{sec:gridsearch}

%-------------------------------------------------%
\subsubsection{Optimized Pipeline Parameters with a Grid Search}\label{sec:gridsearch}
%-------------------------------------------------%

\begin{table}
\caption{Grid Search of Pipeline Parameters}
\begin{center}
\begin{tabular}{cccccc}
\hline \\ [-0.2cm]
N$_{orient}$ & Pixels/Cell & Cells/Block & C$_{reg}$ & HOG size & Score \\ [0.2cm]
\hline \\ [-0.2cm]
\multicolumn{6}{c}{(a) HST-like data} \\ [0.2cm]
\hline \\ [-0.2cm]
9 & (4, 4) & (3, 3) & 50. & &$0.76181\pm0.00491$\\ [0.2cm]
9 & (4, 4) & (1, 1) & 50. & &$0.83403\pm0.00547$\\ [0.2cm]
9 & (8, 8) & (3, 3) & 50. & &$0.85417\pm0.00851$\\ [0.2cm] 
9 & (8, 8) & (1, 1) & 50. & &$0.86389\pm0.00687$\\ [0.2cm] 
9 & (16, 16) & (1, 1) & 50. & & $0.88542\pm0.00680$ \\ [0.2cm]
9 & (16, 16) & (3, 3) & 10. & &$0.89306\pm0.00687$ \\ [0.2cm]
8 & (16, 16) & (3, 3) & 50. & &$0.90000\pm0.00170$ \\ [0.2cm]
9 & (16, 16) & (3, 3) & 50. & &$0.90139\pm0.00804$ \\ [0.2cm]
\hline \\ [-0.2cm]
\multicolumn{6}{c}{(b) LSST-like data} \\ [0.2cm]
\hline \\ [-0.2cm]
& & & & &\\
& & & & &\\ [0.2cm]
\hline
\end{tabular}
\end{center}
\label{tab:gridsearch}
\tablecomments{Panel (a) shows the results of a grid search across a
  range of HOG parameters and regression coefficient \wording{with
    training and test sets of size 1440 with the HST-like mock
    dataset.}  Panel (b) shows the corresponding results for training
  and test sets of \wording{size 5760 with the LSST mock dataset.}  We
  explore different HOG parameters in each dataset due to resolution
  and image size differences.}
\end{table}

We run a grid search across parameters that should reasonably sample
the arc edges in either the HST- or LSST-like mock observations, and
illustrate the results in Table~\ref{tab:gridsearch}.  Recall, the
HST-like images are $300\times300$ pixels per image, while the
LSST-like mock observations are $45\times45$ pixels per image.

We first estimate the size of a cell that will contain a coherent arc
feature.  To first order approximation, subdivisions of cells that are
1/100th the area of the entire image should contain coherent arc edges
that span an elongated shape within arc-containing cells.  Therefore,
we sample the pixels per cell parameter from (8, 8) to (32, 32) for
the HST-like images, and (3, 3) to (5, 5) for the LSST-like images in
our grid search.

Next, the cells per block parameter determines the normalization of
each cell with respect to the neighboring cell.  In general, this will
downweight arc-like edges in cells that neighbor very bright cells,
such as cells that cover the central lens galaxy.  We therefore vary
the cells per block parameter between (2, 2) and (4, 4) for the
LSST-like images and between (1, 1) and (4, 4) for the HST-like
images.

The number of orientations will determine the sampling of rounded
edges.  For example, if we only have two orientations, an arc-like
feature in a cell directly north of the lensing galaxy will appear in
our HOG visualization as a strong horizontal line (e.g. see top left
in Figure~\ref{fig:hogimages}), and an arc-like feature north-east of
the lensing galaxy will appear as an L-shape.  However, contributions
from a cluster or line-of-sight galaxy in the same cell will tend to
contribute edges in all orientations of the histogram (e.g. bottom
right in Figure~\ref{fig:hogimages}).  

Finally, the resolution of the overall image will also limit the
additional information that an increase in $N_\text{orient}$ will
provide.  \wording{For the LSST-like data, the optimal orientation is
  $N_\text{orient}=?$ compared with $N_\text{orient}=?$ only results
  in a $???\%$ improvement.}

% Impact on image performance
The image size affects the length of the HOG feature vector, which has
a monotonically increasing relationship with the time required to
train the model.  Additionally, for fixed memory restrictions, there
is a tradeoff between the length of the feature vector and the size of
the training set.  We will discuss how the training set size affects
the train time for each data set in Section~\ref{sec:trainsetsize}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\begin{figure*}[t]\label{fig:hogimages}
\begin{center}
\includegraphics[width=2\columnwidth]{figures/tiles/compilations/hog_telescope_compilation.pdf}
\caption{Left to right show example image transforms of mock images
  from Figure~\ref{fig:mockimages} with a visualized histogram of
  oriented gradients.  The image transform picks up edge features,
  with arc features showing up as edges across radial orientations.
  \wording{Each of the oriented gradients within a cell is
    color-coded by magnitude, and represented as a line in the
    direction perpendicular to that gradient.}  The actual extracted
  features fill a one-dimensional feature vector comprised of the
  magnitudes of each of the oriented edges within the visualized
  cells.}
\end{center}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            
%% \caption{Top row: Visualized HOG feature vectors of the sample
%%   lensed HST-like image from Figure~\ref{fig:mockimages}.  Left:
%%   The corresponding HOG visualization with N$_\text{orient}=5$,
%%   (32, 32) pixels per cell, and (1, 1) cells per block.  Right: The
%%   HOG visualization of the same mock image with
%%   N$_\text{orient}=9$, (16, 16) pixels per cell, and (1, 1) cells
%%   per block.  The brightest lines align with prominent edges.
%%   Note, while fewer pixels per cell will better resolve the arc
%%   feature, the ``edge'' corresponding to the arc is less prominent.
%%   Bottom row: Visualized HOG feature vectors for the LSST-like
%%   single epoch and stacked images with the best scoring HOG
%%   parameterization of N$_\text{orient}=4$, (3, 3) pixels per cell,
%%   and (3, 3) cells per block.  The single epoch image (left) does
%%   not have significant enough edge features to emerge in the HOG,
%%   but the stacked image (right) does.}  \end{center} \end{figure*}
%%   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%-------------------------------------------------%
\subsection{Machine Learning Algorithm: Logistic Regression}\label{sec:LR}
%-------------------------------------------------%
The problem of detecting gravitational lenses in images falls under
the general category of {\em{classification}} in machine learning. In
general the task is to find a function that assigns data points $x$ to
one of two or more classes, denoted by the class label $y$.  This is
equivalent to specifying a decision boundary, or decision boundaries
between the classes in the space of the data points.  (Compare this to
{\em{regression}} in which the task is to find a function $y=f(x)$,
where $y$ is a continuous, rather than discrete, variable.) In our
case, we have two classes: lens and non-lens containing images, and
the data points $x$ are the HOG feature vectors transformed from the
training set data.  In this paper we use the Logisitic Regression (LR)
algorithm, for which the decision boundary is a hyperplane .  (The
equivalent in the regression setting would be linear regression.) In
LR we determine the optimal hyperplane by minimizing the objective
function
\begin{equation}
\label{eqn:log_reg_objective}
L(A,b) = \sum_i \log \left[1 + \exp \left(-y_i (A \cdot x_i + b) \right) \right]
\end{equation}
where $x_i$ is a data point (HOG feature vector), $y_i$ is the
known label for that data point ($1$ for a lens containing
image, $-1$ for a non-lens containing), and $A$ and $b$ are
the parameters of the hyperplane. Eq. \ref{eqn:log_reg_objective}
is to be minimized with respect to $A$ and $b$.
Other more complicated machine learning algorithms exist which
do not necessarily produce a linear decision boundary, such as
Support Vector Machines (SVM), Random Forests, and Neural Networks.
training set data \citep{hastie_09}.

The HOG feature vectors in this paper can be very high-dimensional.
When dealing with high-dimensional data, where the number of
dimensions becomes comparable to the number of data points,
overfitting can become an issue. (An example of an extreme case
of overfitting would be if you fit a degree $n$ polynomial to $n$
points.  The polynomial would simply wiggle so that it goes
through every point, and would have no predictive power if you
tried to interpolate or extrapolate.) In machine learning,
overfitting is avoided using different {\em{regularization}}
techniques. A common choice for logistic regression is to
add a penalty term to Equation~\ref{eqn:log_reg_objective}
\begin{equation}\label{eqn:log_reg_regularized}
L_\text{Reg}(A,b,C_\text{LogReg}) = L(A,b) + \frac{1}{2C_\text{LogReg}} \|A\|
\end{equation}
where the norm $\|A\|$ is typically taken as either the
sum of squares of the coefficients ($L_2$ norm) or the sum
of absolute values of the coefficients ($L_1$ norm).  In this
paper we use the former.

The amount of regularization is controlled by the parameter
$C_\text{LogReg}$: larger values of $C_\text{LogReg}$ correspond to
increasing model complexity.  If $C_\text{LogReg}$ is too large then
the model will overfit, and if it is too small the model will
underfit.  To determine whether a model is overfit or underfit, the
model is trained, (i.e. Eq. \ref{eqn:log_reg_regularized} is
minimized), on a subset of the data called the {\em{training set}} and
its performance (goodness of fit) is evaluated both on the the
training set and a set of hold-out data called the {\em{test set}}
that was not used in constructing the model.
Figure~\ref{fig:regularization} shows the performance of a model for
selected HOG parameters as a function of the regularization parameter,
$C_\text{LogReg}$, for both the training and the test set.  (The
performance can be measured by the accuracy, i.e. percent of images
correctly classified, or by some other metric, such as the area under
the Receiver Operating Characteristic curve (see
Section~\ref{sec:ROC}).)

When $C_\text{LogReg}$ is small, the performance of the model improves
with increasing $C_\text{LogReg}$ on both the training and test set,
meaning that $C_\text{LogReg}$ is
still in the underfitting regime. Eventually the performance on the
test set reaches a maximum and starts to decrease, even while the
performance on the training set continues to increase.  This means
that the model is no longer generalizing well and is starting to
overfit. The optimal $C_\text{LogReg}$ occurs when the performance
of the test set is at its maximum; this is the value of $C_\text{LogReg}$
that should be used in the final model.

In practice, there is something of a trade-off between accuracy and
computational resources because a larger value of $C_\text{LogReg}$
will also increase the training time, since a larger $C_\text{LogReg}$
corresponds to a less constrained parameter space being searched
\todo{(citation needed)}.
We discuss the performance and training time dependence on $C_\text{LogReg}$
in Section\ref{sec:regularization}.

%-------------------------------------------------%
\section{Results}
\label{sec:results}
%-------------------------------------------------%

%-------------------------------------------------%
\subsection{Receiver Operating Characteristic}\label{sec:ROC}
%-------------------------------------------------%

In this section, we discuss the Receiver Operating Characteristic
(ROC) curve (see Figure~\ref{fig:ROCcompilation}), which shows the
true positive rate (tpr) as a function of false positive rate (fpr)
for a given model and test set.  The true positive rate is defined as
the number of true positives divided by the total number of positives
(completeness of the positive identifications).  The false positive
rate is defined as the number of false positives divided by the total
number of negatives (impurity of the identifications).  The ROC curve
illustrates the performance of our trained model as we vary the
discrimination threshold.

The classifier model assigns a score to each test image, which is a
probability that the image is a strong lensing system.  To construct
the ROC curve, we rank the test images by probability, and calculate
the tpr and fpr for decreasing discrimination threshold.

For very high discrimination threshold, we have maximal purity, but a
low completeness (bottom left region of the ROC).  For a very low
discrimination threshold, we have maximal completeness, but minimal
purity (top right region of the ROC).  The ideal model would have an
ROC curve with data points that go from $(x, y) = (0, 0)$ to $(x, y) =
(0, 1)$ to $(x, y) = (1, 1)$.

In the context of strong lensing systems, we wish to maximize the true
positive rate so we have a representative count of the fraction of
strong lensing systems in an observed volume of the universe.  We also
want to minimize the false positive rate.  Positively identified
strong lensing systems will require expensive spectroscopic follow-up
for validation.  The steepness of the ROC curve indicates how well the
model will optimize the two, and we can characterize the steepness by
the area under the curve (AUC).  The ideal model would have an AUC of
1.  We show the ROC curves of our best performing models in each
dataset.

% Precision-recall
For comparison, we also show a Precision-Recall curve (see
Figure~\ref{fig:PRcompilation}), where precision is the number of true
positives identified as positive divided by the total number of
positively identified images (purity) and recall is the number of true
positives identified as positive divided by the total number of true
positives (completeness).  Each point in the figure is calculated with
a varying threshold for identification.  Since our sample has a class
balance of 50-50 between positives (lens) and negatives (non-lens),
the most lenient threshold for identification would yield a precision
of 0.5 at a recall of 1.0.  It is important to note that this figure
changes as the class balance changes; if we had 90\% non-lenses and
10\% lenses, the most lenient threshold would yield a precision of 1/9
at a recall of 1.  The ROC curve is a more typical metric for
supervised classification.

% Summary of ROC curves
Figure~\ref{fig:ROCcompilation} shows the ROC curves for models
trained using the entire 10,000 training sample, with best-case HOG
and regularization parameters.  The models have been evaluated on a
hold-out test set of 1,000 images that were not used in the parameter
search.  We show the mock HST, LSST1, and LSST10 results respectively
in red, blue, and green.  Solid lines correspond to a model trained
and tested on images with the lensing galaxy.  Dashed lines correspond
to a model trained and tested on images where the lensing galaxy is
excluded from the mock observation, simulating an ideal modeling and
subtraction of the lensing galaxy, which has been one proposed method
to improve the identification of strong lensing systems.  The
corresponding AUC is listed in the legend.

% Model performance summary
The model performance for the mock HST data is AUC=0.975 for images
with the lens galaxy, and AUC=0.98 for images without the lens galaxy
(red solid and dashed).  On the other hand, the model for our
LSST-like dataset for one year has an AUC=0.625 with the lens galaxy
and AUC=0.579 without the lens galaxy (blue solid and dashed), and the
model for our LSST-like dataset for 10 years has an AUC=0.809 with the
lens galaxy and AUC=0.792 without the lens galaxy (green solid and
dashed).  Removal of the lens galaxy does not systematically perform
better, and is actually dependent on the size of the training set.  We
discuss relative model performance and complexity for images with and
without the lens galaxy in Section~\ref{sec:trainsetsize}.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\begin{figure}[t]
\begin{center}
\includegraphics[width=\columnwidth]{figures/metrics/ROC/ROC_compilation.pdf}
\caption{Red, blue, green: ROC curves for models trained on our whole
  10,000 training set and tested on our holdout set of 1,000.  These
  respectively correspond to the HST, LSST 1 year, and LSST 10 year
  data.  The solid lines are for data that include the lensing central
  galaxy, and the dashed lines for the data where there is no lensing
  galaxy, mimicking an ideal removal of the lens.  Model performance
  can be summarized by the area under the curve (AUC), labeled in the
  legend. $AUC=1$ is a perfect model, and $AUC=0.5$ is a useless
  model.}\label{fig:ROCcompilation}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\begin{figure}[t]
\begin{center}
\includegraphics[width=\columnwidth]{figures/metrics/PR/PR_compilation.pdf}
\caption{Red, blue, green: Precision-recall (PR) curves for models
  trained on our whole 10,000 training set and tested on our holdout
  set of 1,000.  These respectively correspond to the same models and
  data shown in Figure~\ref{fig:ROCcompilation}.  An ideal model would
  reach both a precision (purity) and recall (completeness) that equal
  1.  Note, that this performance describes a data set with a 50-50
  split between lens and non-lens containing
  images.}\label{fig:PRcompilation}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            

%-------------------------------------------------%
\subsection{Effects of Regularization on Model Performance}\label{sec:regularization}
%-------------------------------------------------% 

% Logistic regression and regularization
As described in Section~\ref{sec:LR}, LR trains a model with
complexity determined by the regularization parameter coefficient,
$C_\text{LogReg}$.  Larger values of $C_\text{LogReg}$ are less
regularized and allow for increased model complexity.  The highest
values of $C_\text{LogReg}$ will better describe features in the
training set.  However, an overly complex model will overfit the
training set at the expense of its performance on any independent test
set.  The regularization parameter ultimately defines the model
performance, and we must perform a parameter search to identify the
optimal value for $C_\text{LogReg}$.

%  Our running over C - score and AUC as characterizations
Figure~\ref{fig:regularization} shows the model performance as a
function of regularization parameter for each data set HST, LSST, and
LSST10.  The solid and dotted blue lines respectively correspond to
the model performance on the test and training set, with the AUC as a
metric for performance.  In red, we show the train time as a function
of $C_{LogReg}$.

% Training behavior with C
As expected, the training set increases and asymptotes with
$C_\text{LogReg}$.  With increasing model complexity, the model better
fits the training data set.  This is analogous to fitting a seventh
order polynomial to seven data points, where the fitting function will
go through every point but will not likely predict additional points.
With increasing model complexity, we are able to better able to
capture features that are generally characteristic of strong lensing
systems with arcs. However, past a certain $C_\text{LogReg}$, the
model performance on the test set decreases or asymptotes, as it has
overfit the training set.  We use the scaling of AUC with
$C_\text{LogReg}$ when training 8,000 out of our full 10,000 training
data set to determine the best value for $C_\text{LogReg}$.  However,
the optimal parameter is also dependent on the size of the training
set (see Section~\ref{sec:trainsetsize}), so this choice is not
generalizable.

% Train time with Creg
For fixed train size, the log of the train time roughly scales
linearly with the log of $C_\text{LogReg}$.  Since lower values of
$C_\text{LogReg}$ correspond to a more regularized model, there is a
smaller volume in hyperparameter space to search for the best fit
coefficients.  The solution, on average, will converge more quickly,
for more regularized models.  The scaling is not purely monotonic
because the fitting still has some randomness associated with the path
it takes to convergence.

% Scores vs. regularization on larger train/test set with comparable
% grid search in parameterization
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\begin{figure*}[t]
\begin{center}
\includegraphics[width=.66\columnwidth]{figures/Creg/HST.pdf}
\includegraphics[width=.66\columnwidth]{figures/Creg/LSST1.pdf}
\includegraphics[width=.66\columnwidth]{figures/Creg/LSST10.pdf}
\caption{AUC of the model with varying LR regularization coefficient
  parameter, $C_{reg}$, used when training the model classifier.  We
  use a subset of the the 10,000 training images to search over the LR
  $C_{reg}$ parameter, training on 8,000 and testing on 1,000.  Each
  panel corresponds to a different mock observation.  From left to
  right: HST, LSST for one year, and LSST for 10 years.  The solid
  blue lines correspond to the AUC of the test set, and the dotted
  blue lines to the AUC of the training set.  To avoid overfitting, we
  choose the smallest parameter for which the AUC of the test set is
  maximal: 5000, 10, and 5000, respectively.  In thin red solid lines,
  we show the train time of the model, which roughly increases in a
  log-log scaling with logistic regression coefficient
  parameter.}\label{fig:regularization}
\end{center}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            

%-------------------------------------------------%
\subsection{Dataset Size Dependence}\label{sec:datasize}
%-------------------------------------------------% 
%-------------------------------------------------% 
\subsubsection{Effects of Training Set Size on Performance and Train Time}\label{sec:trainsetsize}
%-------------------------------------------------% 

In this section, we show the effects of training set size on model
performance on the holdout test set of 1,000 images.  We also compare
the improvement between images that include the lens galaxy and images
with no lens galaxy.

Figure~\ref{fig:trainsizeLSST10} shows how the AUC and the optimal
logistic regression coefficient depend on the log of the size of the
training set for both the LSST10 data in solid blue lines, and the
nLSST10 data in dashed blue lines.  The AUC for models trained on the
LSST10 data improves almost linearly with the log of the training set
size, increasing from AUC=0.705 to AUC=0.788 when the train size is
increased from 500 lens/non-lens images to 8000 lens/non-lens images.
However, for the nLSST10 data, where the lens galaxy has been removed
from the images, the improvement is less dramatic.  With the same
increase in training size, the AUC for nLSST10 changes from just below
0.77 to just below 0.78.

In solid red, we see that the optimal value for $C_\text{LogReg}$ for
LSST10 roughly scales logarithmically with the log of the train size,
with an exception of the data point corresponding to train size of
2000.  Generally, a larger training set will require an increase in
model complexity to capture additional information.  This is also true
for the optimal $C_\text{LogReg}$ dependence on the number of training
images in the nLSST10 data.  But, the required complexity is
systematically less than for the LSST10 images.  

% Comparison of the two LSST10 vs. nLSST10
In the LSST10 case, the trained model can incorporate the additional
information of the edges from the lens galaxy, which is correlated
with the lensing cross-section and likelihood of the image being a
lensing system.  The nLSST10 images do not contain this information,
but provide cleaner signals of the lensed image for lensed images that
occur close to the lens galaxy.  The cleaner signal in nLSST10 allows
for better model performance for smaller training data set sizes
($N_\text{train}\sim5,000$).  However, models trained on LSST10
improve more rapidly with train size, since the additional information
from the lens galaxy better describes the lens containing images.  For
$N_\text{train}\gsim 5,000$, the models trained on LSST10 outperform
  those trained on nLSST10.

%M Model complexity, train time, and performance
Since the models that contain information from the lens galaxy edges
in LSST10 are more complex, the models require a larger training set
size for a better fit.  While model performance for LSST10 appears to
steadily increase, this comes at the cost of increased train time,
which is two-fold.  The train time will increase due to both an
increase in data to fit, and also an increase in optimal
$C_\text{LogReg}$ where the volume of hyperparameter space for allowed
solutions is larger (see red lines in
Figure~\ref{fig:regularization}).

% Train time
Figure~\ref{fig:traintimeLSST10} shows the train time for LSST10 and
nLSST10 as a function of the size of the training set for the optimal
regularization parameter for that subset training data.  Each model
uses features extracted with the same HOG parameterization from the
grid search and the optimal regularization parameter for that subset
train data.  The train time of a given model generally increases for
increasing regularization parameter.  For the subset of train size
$N_\text{train}=2000$ in LSST10, the optimal regularization parameter
happened to be $C_{LogReg}=100$, whereas it was $C_{LogReg}=500$ for
the subset of trainsize 1000, and $C_{LogReg}=1000$ for the subset of
trainsize 4000.  This makes the train time increase at train size
$N_\text{train}=2000$ for LSST10 less dramatic than the average
log-log slope of approximately 2.

% Train time behavior for nLSST10
Since nLSST10 does not contain the lens galaxies, fewer of the
extracted features describe the lensing system, requiring decreased
model complexity.  The increase in train time for nLSST10 as a
function of train size is mostly due to only having more data to fit
in the regression, leading to a steady and slow increase of train time
with number of training images with log-log slope of approximately 1.

% AUC vs. regularization on larger train/test set with comparable
% grid search in parameterization
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\begin{figure}[t]
\begin{center}
\includegraphics[width=\columnwidth]{figures/trainsize/LSST10_with_nLSST10.pdf}
\caption{Solid (dotted) blue line: AUC for models with varying size of
  the training set for LSST10 (nLSST10).  Solid (dotted) red line:
  Optimal regularization parameter for training on the train size for
  LSST10 (nLSST10). The improvement of AUC scales linearly with the
  log of the training set size, but train time roughly scales
  logarithmically.  Note, LSST10 requires more model complexity to
  exceed the performance of nLSST10.}\label{fig:trainsizeLSST10}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\begin{figure}[t]
\begin{center}
\includegraphics[width=\columnwidth]{figures/traintime/LSST10.pdf}
\caption{Solid (dotted) blue line: Train time for models with varying
  size of the training set for LSST10 (nLSST10).  Train time roughly
  scales logarithmically.  Note, LSST10 requires more model complexity
  to exceed the performance of nLSST10, and therefore requires more
  training time for continual increase in
  performance.}\label{fig:traintimeLSST10}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            

%-------------------------------------------------% 
\subsubsection{Effects of Rotation on AUC}
%-------------------------------------------------% 

% Augmenting dataset
To augment our training set, we rotated each image in the set by
multiples of $90^o$.  Augmenting our data by a factor of four
naturally optimizes the use of available training data, since our
feature extraction method of HOG is not rotationally invariant.  This
has an equivalent improvement to the study illustrated in
Figure~\ref{fig:trainsizeLSST10}.

% Rotations of the test set
We also tested the effects of evaluating our model on all four
rotations of the test set, and using the average score of each test
image to calculate the AUC.  In Figure~\ref{fig:rotation_test}, we
show the AUC for different orientations of the datasets.  The x-axis
corresponds to each of our three datasets, with and without the lens
galaxy.  The y-axis shows the AUC.  The filled blue circles correspond
to the four AUCs calculated when the model evaluated images at each of
the four rotations.  The filled red stars correspond to the AUC
calculated from the average of all four test scores, which are
systematically higher than any one rotation.  The average score across
all rotations for each image is likely to be less noisy for the whole
test sample, giving an improved AUC.

% Summary figure
Figure~\ref{fig:rotation_test} also summarizes the best-case results
of our models trained on our entire 10,000 training sets, and tested
on our holdout 1,000 test set.  Recall, however, that we expect model
performance on images containing the lens galaxy to improve further
with larger training sets (see Figure~\ref{fig:trainsizeLSST10}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\begin{figure}[t]
\begin{center}
\includegraphics[width=\columnwidth]{figures/rotated_aucs/rotated_aucs.pdf}
\caption{Our summary figure: The AUCs of models trained on the full
  10,000 and tested on the holdout 1,000.  Blue circles: AUC
  calculated from scores of images at a given rotation (e.g. 0, 90,
  180, and 270 degrees).  Red stars: AUC calculated from the average
  score of all rotations of each image.  The average score produces an
  improved AUC in all data sets.  We expect the AUC to further improve
  with increased train size.}\label{fig:rotation_test}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            



%-------------------------------------------------%
\subsection{Image classification performance}\label{sec:performance}
%-------------------------------------------------%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\begin{figure*}[t]\label{fig:ROCsamplesLSST10}
\begin{center}
\includegraphics[width=.8\columnwidth]{figures/tiles/compilations/LSST10/tp.pdf}\hspace{5pt}
\includegraphics[width=.8\columnwidth]{figures/tiles/compilations/LSST10/fp.pdf}\\\vspace{5pt}
\includegraphics[width=.8\columnwidth]{figures/tiles/compilations/LSST10/bp.pdf}\hspace{5pt}
\includegraphics[width=.8\columnwidth]{figures/tiles/compilations/LSST10/bn.pdf}\\\vspace{5pt}
\includegraphics[width=.8\columnwidth]{figures/tiles/compilations/LSST10/fn.pdf}\hspace{5pt}
\includegraphics[width=.8\columnwidth]{figures/tiles/compilations/LSST10/tn.pdf}\\
\caption{LSST 10 year mock images. Left two columns: Lens containing
  images, annotated with the image score assigned by our trained
  classifier.  Right two columns: Non-lens containing images,
  annotated with the image score.  The top two rows show
  characteristic images that will be accepted with a high threshold
  for classification, contributing to the bottom left of the ROC curve
  in Figure~\ref{fig:ROCcompilation}.  The middle two rows show
  characteristic images that will be accepted with a moderate
  threshold, contributing to the knee of the ROC curve, with true
  positive rates and false positive rates of $tpr\approx0.8$ and
  $fpr\approx0.2$, respectively.  The bottom two rows show
  characteristic images that will only be accepted with an extremely
  lenient threshold, contributing to the top right area of the ROC
  curve.}
\end{center}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            




**Redshift makes sense, but only because of the redshift distribution.
--> Zahid+15 quiescent.... this is where redshifts are from.

% Defining the 6 paradigms of classification
In this section, we discuss the different image types that our model
is most and least able to successfully classify.  We have six
paradigms of model performance on the mock images, \todo{and include a
larger compilation of each of the six image classification paradigms
in the Appendix for the various models we have trained}.

% TP and TN
Figure ~\ref{fig:HSTclassificationtile} shows a tile of representative
HST-like images classified with the model, HST:N5ppc32cpb4C100.  The
left column show lensing systems, and right column non-lensing
systems.  From top to bottom, the images are ordered from highest
scoring to lowest scoring with the classification model.  The top left
is what we would call a ``true positive'', a lensing system that will
always classified as positive because of its high score.  Most of the
highest scoring lensing systems exhibit similar extended arcs
corresponding to a large magnification (see
Section~\ref{sec:peformance_lensmodel}).  The bottom right is a ``true
negative'', a non-lensing system that will likely be classified as
negative because of its low score.  Many of the lowest scoring
non-lensing systems have \todo{fairly small BCGs, which are non likely
  to lens a background galaxy.}

% FN and FP
The bottom left corresponds to a likely ``false negative,'' or a
lensing system that is likely to be mis-classified as a negative.  The
lowest scoring lensing systems tend to have images with smaller
magnification that are minorly distorted, mimicking the shape of
line-of-sight galaxies that the model has learned to ignore.  The top
right corresponds to a likely ``false positive,'' or a non-lensing
system that is likely to be mis-classified as a positive.  False
positives tend to have ``fuzzy'' along the line of sight galaxies that
mimic lensing arcs.  The false positives should be from bright
galaxies along the line of sight.  \todo{The highest scoring
  non-lensing systems often have fairly bright disk shaped galaxies
  along the line of sight.  Visually, these are also virtually
  indistinguishable from a lensed image and would require
  spectroscopic follow-up.}

% BP and BN
The middle row are images whose classification will be most sensitive
to the classification threshold.  We call these borderline images,
with the left corresponding to a ``borderline positive'', and the
right corresponding to a ``borderline negative''.  They sit around the
knee of the ROC curve, the optimal threshold for purity and
completeness.  Note that the ROC curve for HST:N5ppc32cpb4C100
illustrates that we can lower the classification threshold and improve
the true positive rate by $\sim10\%$ and sacrifice an increase in
false positive rate by $\lsim5\%$.  However, small perturbations
around the knee will either include or disinclude the middle row
images.  These are the systems that would make good candidates for
visual and/or spectroscopic follow-up.

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
%% \begin{figure*}[t]
%%   \centering \mbox{
%%     \subfigure{\includegraphics[width=\columnwidth,trim=120 50 0 0,
%%         clip]{figures/HST/1787_lensed_imgs_1.pdf}}\hfill
%%     \subfigure{\includegraphics[width=1\columnwidth,trim=120 50 0 0,
%%         clip]{figures/HST/1354_unlensed_imgs_1.pdf}} } \mbox{
%%     \subfigure{\includegraphics[width=\columnwidth,trim=120 50 0 0,
%%         clip]{figures/HST/1052_lensed_imgs_1.pdf}}\hfill
%%     \subfigure{\includegraphics[width=1\columnwidth,trim=120 50 0 0,
%%         clip]{figures/HST/1994_unlensed_imgs_1.pdf}} } \mbox{
%%     \subfigure{\includegraphics[width=\columnwidth,trim=120 50 0 0,
%%         clip]{figures/HST/1862_lensed_imgs_1.pdf}}\hfill
%%     \subfigure{\includegraphics[width=1\columnwidth,trim=120 50 0 0,
%%         clip]{figures/HST/1723_unlensed_imgs_1.pdf}} }
%% \caption{HST-like images. The left column corresponds to typical
%%   lensing systems in our sample from highest scoring (top) to lowest
%%   scoring, or false negatives (bottom).  The right column corresponds
%%   to typical non-lensing systems in our sample from highest scoring,
%%   or false positives (top), to lowest scoring (bottom).  The middle
%%   row can be considered ``borderline'' classified images, where the
%%   scores of the lensing and non-lensing system are very close to one
%%   another.  These are systems that will be most sensitive to the
%%   classification threshold.}
%% \label{fig:HSTclassificationtile}
%% \end{figure*}
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
%% \begin{figure*}[t]
%%   \centering 

%% \mbox{
%%     \subfigure{\includegraphics[width=\columnwidth,trim=120 50 0 0,
%%         clip]{figures/LSST/BCG/lsst_stack_1204_lensed_imgs_1.pdf}}\hfill
%%     \subfigure{\includegraphics[width=1\columnwidth,trim=120 50 0 0,
%%         clip]{figures/LSST/BCG/lsst_stack_1534_unlensed_imgs_1.pdf}}
%% } 

%% \mbox{
%%     \subfigure{\includegraphics[width=\columnwidth,trim=120 50 0 0,
%%         clip]{figures/LSST/BCG/lsst_stack_1384_lensed_imgs_1.pdf}}\hfill
%%     \subfigure{\includegraphics[width=1\columnwidth,trim=120 50 0 0,
%%         clip]{figures/LSST/BCG/lsst_stack_1703_unlensed_imgs_1.pdf}} 
%% } 

%% \mbox{
%%     \subfigure{\includegraphics[width=\columnwidth,trim=120 50 0 0,
%%         clip]{figures/LSST/BCG/lsst_stack_1641_lensed_imgs_1.pdf}}\hfill
%%     \subfigure{\includegraphics[width=1\columnwidth,trim=120 50 0 0,
%%         clip]{figures/LSST/BCG/lsst_stack_1518_unlensed_imgs_1.pdf}} 
%% }

%% \caption{LSST-like images with a BCG. These illustrate the same image
%%   classification paradigms as Figure~\ref{fig:HSTclassificationtile}.
%%   Note, the larger PSF of LSST reduces the resolution of images,
%%   making the arc-like morphology of faint images less significant and
%%   increases the appearance of image-BCG blending.}
%% \label{fig:LSSTBCGclassificationtile}
%% \end{figure*}

%% \begin{figure*}[t]
%%   \centering 

%% \mbox{
%%     \subfigure{\includegraphics[width=\columnwidth,trim=120 50 0 0,
%%         clip]{figures/LSST/noBCG/lsst_stack_1957_lensed_imgs_0.pdf}}\hfill
%%     \subfigure{\includegraphics[width=1\columnwidth,trim=120 50 0 0,
%%         clip]{figures/LSST/noBCG/lsst_stack_stamps_1098.pdf}}
%% } 

%% \mbox{
%%     \subfigure{\includegraphics[width=\columnwidth,trim=120 50 0 0,
%%         clip]{figures/LSST/noBCG/lsst_stack_1634_lensed_imgs_0.pdf}}\hfill
%%     \subfigure{\includegraphics[width=1\columnwidth,trim=120 50 0 0,
%%         clip]{figures/LSST/noBCG/lsst_stack_stamps_1664.pdf}} 
%% } 

%% \mbox{
%%     \subfigure{\includegraphics[width=\columnwidth,trim=120 50 0 0,
%%         clip]{figures/LSST/noBCG/lsst_stack_1629_lensed_imgs_0.pdf}}\hfill
%%     \subfigure{\includegraphics[width=1\columnwidth,trim=120 50 0 0,
%%         clip]{figures/LSST/noBCG/lsst_stack_stamps_1399.pdf}} 
%% }

%% \caption{LSST-like images with no BCG. These illustrate the same image
%%   classification paradigms as Figure~\ref{fig:HSTclassificationtile}.}
%% \label{fig:LSSTnoBCGclassificationtile}
%% \end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            
\subsection{Performance dependence on lens-model parameters}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
%-------------------------------------------------%
\subsubsection{Effects of Image Magnification on Scoring}
\label{sec:peformance_lensmodel}
%-------------------------------------------------%

% Summary of subsection
Here, we examine lens-model parameters that affect how well our
pipeline can classify the system.  The lens-model parameters we
examined are the redshift, ellipticity, orientation angle, and
velocity dispersion of the lensing BCG, and also the magnification of
the lensed image compared with its original size.  The magnification
of the lensed image is the most correlated lens-model parameter with
our trained model performance.  The more strongly lensed an image is,
the larger its magnification, and the easier it is for our trained
model to classify.

% Discuss figure
In Figure~\ref{fig:scorevslensparams}, we show the classification
score as a function of image magnification for our HST-like sample.
The left-hand side shows the relation color-coded by the redshift of
the lensing galaxy, and the right-hand side shows the relation
color-coded by the velocity dispersion of the lensing galaxy.  Lensing
systems with images that have magnification $\gsim7$ will almost
certainly be classified as positive with threshold scores above
$\gsim0.5$.  These systems are also those that are most easily
classified by eye.  However, our trained model has varying performance
for systems with lower magnifications.

% Some dependence on redshift and velocity dispersion, which is folded
% into magnitude
Almost \todo{X\%} of lensing systems with magnifications $\lsim7$ will
be falsely identified as non-lenses for classification threshold
scores $\gsim0.5$.  These systems typically have lenses that are at
higher redshifts of $z\gsim0.5$ (lighter colors in left figure) and/or
velocity dispersions lower than $\sigma_v\lsim230.$~km/s (darker
colors in right figure).  Note that lensing galaxies with smaller
velocity dispersions are less massive and therefore have a smaller
efficiency of lensing cross section.  The smaller efficiency means
that a background galaxy is less likely to be strongly lensed with
high magnification.  Also, due to hierarchical structure formation,
galaxies are less massive at higher redshifts, so the trends of model
performance with these three parameters are somewhat degenerate with
one another.  

The relationship between our model performance and these lens
parameters indicates that the magnification of lensed images is the
most relevant, and the distribution of image magnification in a data
set will impact trained model performance.  We did not find
correlations in model performance with other lens parameters.  It is
also useful to keep in mind that lensed galaxy images with
magnification $\lsim5$ (e.g. slightly stretched spherical galaxies)
are often visually indistinguishable from elliptical galaxies along
the line of sight.  \todo{We will discuss how differences between the
  lens-model parameter distributions in the training and test sets
  affect performance in a forthcoming paper.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\begin{figure*}[t]
  \centering 

\mbox{
    \subfigure{\includegraphics[width=\columnwidth,trim=0 0 0 0,
        clip]{figures/HST/or5ppc32cpb4/magnitude_d_colorcodedby_z.pdf}}\hfill
    \subfigure{\includegraphics[width=\columnwidth,trim=0 0 0 0,
        clip]{figures/HST/or5ppc32cpb4/magnitude_d_colorcodedby_velocity_dispersion.pdf}}
} 
\caption{Image classification score as a function of lensed image
  magnification.  Left color-coded by redshift, right is color coded
  by the lens velocity_dispersion.}
\label{fig:scorevslensparams}
\end{figure*}

%-------------------------------------------------%
\subsubsection{Performance sensitivity to line-of-sight galaxies}
\label{sec:peformance_lensmodel}
%-------------------------------------------------%



%-------------------------------------------------%
\subsection{Effects of BCG contamination}
%-------------------------------------------------%

Previous works to identify strong lensing systems have often modeled
out the BCG from images to better identify lensed images (???CITE
HERE???).  Here, we test the effects of the BCG by training and
testing models on mock data where the BCG has not been included.  By
construction, these results show the maximal improvement that can be
obtained from BCG subtraction.

Figure~\ref{fig:ROCnobcg} shows the \todo{ROC curve for models of HOG
  feature vectors of mock data with no BCG.  The feature vectors are
  the same HOG parameters as in Figure~\ref{fig:ROC} }.  \todo{The
  grid search over parameter space was similar to that for data with
  the BCG.}

???????

%-------------------------------------------------%
\section{Summary and Discussions}
\label{sec:conclusions}
%-------------------------------------------------%

machine learning -> pipeline, and training set.  

Future work with training set tests.


We summarize key points below:

%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{itemize}

\item We have designed our pipeline to easily select and add image
  pre-processing and feature extraction methods, and to select a
  machine learning algorithm for classification.  Additionally, the
  user can easily perform parameter searches to train a model with the
  best parameters for a given problem.

\item We have tested HOG as an efficient and effective feature
  extractor for galaxy-galaxy strong lensing systems in both a space
  based (HST-like) and ground based (LSST-like) observation.  

\item We have found almost no difference between the Logistic
  Regression and Support Vector Machine algorithms, with Logistic
  Regression performing at a factor of $XX$ faster.  This means that
  features of lens and non-lens images are relatively well separated
  by hyperplanes in feature space.

\item We find AUC values of ROC curves of optimized classifier models
  to yield $AUC=0.987$ for the HST-like data, $AUC=0.96$ for the
  stacked LSST-like data with the lensing BCG included, and $AUC=0.86$
  for the stacked LSST-like data with the lensing BCG perfectly
  removed.  This suggests that we will be able to use our method to
  detect strong galaxy-galaxy lensing systems in future space-based
  missions (e.g. {\em Euclid}) with high purity and completeness.
  However, for future ground-based missions like LSST, we will need to
  perform additional image pre-processing, such as subtracting the
  image component corresponding to the lens.

\item Images that were easiest for our model to classify typically
  were lens systems that had high magnification and a BCG with large
  velocity dispersion.  

\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%

These results indicate that...

% Caveat of the mock data set
One major caveat ... training and test mock images... LOS galaxies all
from Hubble Ultra Deep field.  Success measured by performance on the
small sample of systems observed with HST.

% Interpreting TP and FP rates
Note that while our training and test set split of $50\%$ lensed and
$50\%$ unlensed images is not realistic, our true positive and false
positive rates in the ROC curves would stay the same for alternative
lensed and unlensed sample splittings.  

On the other hand, the ROC curves show a representative rate for lens
and source distributions that are evenly sampled.  The sampling is
{\it not} representative of what we might expect from a an
observational survey.


\acknowledgments CA acknowledges support from the Enrico Fermi
Institute at the University of Chicago, and the University of Chicago
Provost's Office. This work was also supported in part by the Kavli
Institute for Cosmological Physics at the University of Chicago
through grant NSF PHY-1125897 and an endowment from the Kavli
Foundation and its founder Fred Kavli.
\lastpagefootnotes


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\section{Sample stacked mock LSST images}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\subsection{BCG included}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        

\bibliography{ms}

\end{document}  

