\documentclass{emulateapj}
\usepackage{amsmath} 
\usepackage{apjfonts} 
\usepackage{amssymb} 
\usepackage{xspace}
\usepackage{hyperref} 
\usepackage{natbib} 
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{epsf}
\usepackage{subfigure}
\bibliographystyle{apj_ads}
\usepackage[usenames,dvipsnames]{xcolor}
\newcommand{\comment}[1]{{\color{red} #1}}
\newcommand{\change}[1]{{\color{blue} #1}}
\newcommand{\todo}[1]{{\bf\color{blue} #1}}
%%%%%%%%%% User defined symbol %%%%%%
\def\gsim{\gtrsim}
\def\lsim{\lesssim}    
\def\Msun{M_\odot}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document} 
\shorttitle{LensFinder}
\shortauthors{Avestruz, Li, Lightman}
\submitted{The Astrophysical Journal} 
\slugcomment{The Astrophysical Journal, submitted}

\title{MachLLensFinder - I: A Machine Learning Strong Lensing
  Identification Pipeline}

\author{Camille Avestruz\altaffilmark{1-3}\thanks{E-mail:
    avestruz@uchicago.edu}, Nan Li\altaffilmark{2-4}, and Matthew Lightman\altaffilmark{5} }

\affil{
$^1${Enrico Fermi Institute, The University of Chicago, Chicago, IL 60637 U.S.A.}\\
$^2${Kavli Institute for Cosmological Physics, The University of Chicago, Chicago, IL 60637 U.S.A.}\\
$^3${Department of Astronomy \& Astrophysics, The University of Chicago, Chicago, IL 60637 U.S.A.};\\
$^4${High Energy Physics Division, Argonne National Laboratory,
Lemont, IL 60439};\\
$^5${Digital Intelligence, JPMorgan Chase, Chicago, IL 60603 U.S.A.};\\
  \href{mailto:avestruz@uchicago.edu}{avestruz@uchicago.edu}\\
}

\keywords{gravitational lensing --- methods : numerical --- methods: data analysis --- methods: statistical --- galaxies: elliptical --- surveys  }
  
%-------------------------------------------------%
\begin{abstract} 
  Gravitational lensing offers a direct probe of the underlying mass
  distribution of lensing systems, a window to the high redshift
  universe, and a geometric probe of cosmological models.  The advent
  of large scale surveys such as the Large Synoptic Sky Telescope and
  Euclid has prompted a need for automatic and efficient
  identification of strong lensing systems.  In this first paper of a
  series, we present LensFinderML, a strong lensing identification
  pipeline that will be publicly released as open source software.

Giant arcs trace... cosmology... mass distribution.  Advent of DES,
LSST, etc. need an efficient way to identify strong lens images.  We
present {\sc{MACHLLENSFINDER}} (MACHine Learning LENSFINDER), an open
source image processing and deep learning.  This first paper focuses
on image Processing Techniques for Giant Arc Identification Tests with
mock images from the Hubble Space Telescope and LSST.  Image
processing improves by....
\end{abstract}
%-------------------------------------------------%

%-------------------------------------------------%
\section{Introduction}
%-------------------------------------------------%

% Gravitational lensing as a geometric (?) test of cosmology.  Also
% probes underlying matter distribution in galaxy clusters, sensitive
% to things like recent accretion and environment.
Gravitational lensing deflections due to gravity... \citep[see][for a
  review]{kneibandnatarajan_11}.  Strong lensing visible ... giant
arcs, multiple images, and arclets.  Galaxy clusters as strong lens,
deep gravitational potential well, can see by eye.  Strong lensing
signatures probe underlying dark matter distribution of galaxy
clusters, magnify the background galaxy population allowing us to
probe formation at epochs further back in time, and geometric test of
cosmology.

% Arc identification can be done visually (e.g. that zooniverse
% thing).  But, in the advent of optical surveys, DES and LSST, we
% will be limited by our ability to automatically process the large
% amounts of data.  Wide field surveys - SPT in millimeter, Herschel
% in submillimeter; high resolution spectroscopic imaging with ALMA;
% optical surveys DES, HSC, LSST, and Euclid (Oguri \& Marshall 2010).
The geometric test of cosmology requires a comparison of predicted arc
abundances and observed abundances.  In the advent of large surveys,
such as DES and LSST, we will be limited by our ability to
automatically process the large amounts of data and select likely
strong lensing clusters for follow up spectroscopic measurements to
positively identify background galaxy images.  Image identification
has had an onslaught of new works (wc).

% Serentipitous discoveries confirmed the theory (cite Einstein)
% Cite first discoveries of different shapes of strong lensing images
% While some serendipitous systems found recently (gladders+),
% Describe the uptick in amount of data
% End with need to scale with the volume of data
Early strong lensing images were serendipitous discoveries
(e.g. multiple quasars \citep{walsh_etal79}, arcs in clusters
\citep{lyndsandpetrosian_86}, ...).  As the number of images with
potential lensing systems increased, infrastructure emerged for both
automated and visual inspection.  

% Image identification works: visual, ``robot'' identification -
% ArcFinder, RingFinder..., and citizen science (space warps).
% Many of these rely on some explicit aspect of the image's morphology (give example), filtering, ... ....
% Specifics of the methodology how the instrument affects the strong lensing signal, and the objects of interest.

Automated lensed image identification has fallen under the term
``robot'' identification, where images are fed into some kind of image
processing pipeline that enhances and extracts characteristic features
as parameters that are fed into some kind of cutoff or pattern
recognition piece.  ArcFinder
... .... \citep{seidelandbartelmann_07}. For multiply imaged quasars,
Ringfinder ... ... ...\citep{gavazzi_etal14}.  Each pipeline
.... ... and relies on the image morphology where ... ...  Note, that
the method best applied to ... ... and the objects of interest.  The
human eye is one of the best discriminators of.... SpaceWarps is an
example of citizen science... visual identification
\citep{marshall_etal16,more_etal16}.  However, scalability, training
....

% Arc finding must optimize for completeness and purity.  Completeness
% (no false negatives), purity (minimal false positives).  Purity can
% be double checked in a smaller subsample of the data visually.  And,
% spectroscopic follow up is expensive.  Also, scalability of the
% strong lens finding method matters - example with the citizen
% science.
In general, lensed image finding must optimize for completeness and
purity.  In the application of arc abundance comparisons with
cosmological predictions, completeness is important; we must be able
to understand the reasons and frequency of false negatives.  Since
spectroscopic confirmation of background lensed images is expensive,
purity is key; we must minimize false positives that would waste
follow up telescope time.

% Machine learning to automate.  Previous works (StrongML),
% More+... Here, we focus on the improvement from preprocessing the
% images and describe the pipeline, which is publicly available.
% Depending on the image type, different preprocessing is likely
% necessary (rings, multiply lensed quasars, etc.) Our goal is to
% create an open source pipeline that is general enough for the user
% to select or add appropriate image processing techniques to augment
% the features of the lensed image, train and test data with different
% machine learning techniques, and to select the sequence of methods
% that maximizes completeness with optimal purity.


% No need to explicitly extract morphological parameters to a pattern
% finding, simply transform the image according to the appropriate
% image preprocessing function, and directly feed this into the
% machine learning ... 

% This is the first in a series of LensFinder papers where we present
% our initial pipeline results with a test... of .... Image
% preprocessing.... same technology to validate watermarks


Our paper is organized as follows.  In Section~\ref{sec:methods} we
briefly describe ...  We present our results in
Section~\ref{sec:results}, and our summary and discussions in
Section~\ref{sec:conclusions}.

%-------------------------------------------------%
\section{Methodology}
\label{sec:methods}
%-------------------------------------------------%

%-------------------------------------------------%
\subsection{Mock Images}
%-------------------------------------------------%

% Summary of HST and LSST images
We use mock {\em Hubble Space Telescope} (HST) and {\em Large Synoptic
  Sky Telescope} (LSST) images generated with PICS (Pipeline for
Images of Cosmological Strong lensing) \citep{li_etal16}.  These
images include cluster-member galaxies and foreground stars.  Half of
the images used are strong lensing systems.  Here, lensed galaxy
images are ray-traced images of actual galaxies extracted from deep
Hubble Space Telescope observations.  The final mock images are
``observed'' with a realistic point spread function corresponding to
modeled detector artifacts for bright stars.  See \cite{li_etal16} for
more details.

Figure ~\ref{fig:mockimages} illustrates a sample mock lensed and
unlensed images from each telescope.  From left to right, the top
panel images respectively correspond to a mock HST lensing and
non-lensing system. The mock HST images are $n_\text{pix}\times
n_\text{pix}=300\times300$ in size.  For the HST-like dataset, many
giant arcs are visually obvious.

The bottom panel images respectively correspond to a mock LSST lensing
and non-lensing system.  These images are $n_\text{pix}\times
n_\text{pix}=45\times45$.  The ground based noise and limited
resolution of the LSST-like dataset make visual giant arc
identification difficult.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\begin{figure*}[t]\label{fig:mockimages}
\begin{center}
\includegraphics[width=1\columnwidth]{figures/105_lensed_imgs_1_color.pdf}\hspace{-20pt}
\includegraphics[width=1\columnwidth]{figures/105_unlensed_imgs_1_color.pdf}\\
\includegraphics[width=1\columnwidth]{figures/lsst_9_lensed_imgs_1_color.pdf}\hspace{-20pt}
\includegraphics[width=1\columnwidth]{figures/lsst_9_unlensed_imgs_1_color.pdf}
\caption{Top panel: Example mock HST images, with $n_\text{pix}\times
  n_\text{pix}=300\times300$.  The left corresponds to a strong
  gravitational lensing system with a visibly obvious giant arc.  The
  right corresponds to a non-lensing system.  Bottom panel: Example
  mock LSST images, with $n_\text{pix}\times n_\text{pix}=45\times45$.
  The resolution and noise of a ground based telescope is noticeably
  worse.  The left corresponds to a strong gravitational lensing
  system, and the right to a non-lensing system.  However, visual
  identification of giant arcs in the LSST images is very difficult.}
\end{center}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            

%-------------------------------------------------%
\subsection{Strong Arc Lensing Identification Pipeline}
%-------------------------------------------------%

To perform our analysis, we have used a variety of tools from {\em
  Scikit-learn} \citep{pedregosa_etal12}.  We outline the
identification pipeline in Figure~\ref{fig:pipeline}.  The first step
of our pipeline consists of a feature extraction stage, where our
feature vector is a {\em histogram of oriented gradients}
\citep{dalalandtriggs_05}.  We describe the method in
Section~\ref{sec:hog}.  We then use a machine learning algorithm to
train a classifier model on a subset of our images.  We have explored
both {\em Logistic Regression} (LR) and {\em Support Vector Machine}
(SVM) machine learning algorithms.  We test our trained model on an
independent subset of the images to assess the model performance.

Both the feature extrator, HOG, and the classifier contain a range of
parameters, which must be tested and optimized for peak model
performance.  We use {\em GridSearchCV} from {\em Scikit-learn} to
select appropriate parameters, and discuss these results in
Section~\ref{sec:gridsearch}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\begin{figure}[t]\label{fig:pipeline}
\begin{center}
\includegraphics[scale=0.55]{figures/supervised_classification_workflow.png}
\caption{** Cartoon of pipeline ** Mention that median filtering generates
visual artifacts that confuse the model. http://www.nltk.org/book/ch06.html}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            

%-------------------------------------------------%
\subsection{Feature Extractor: Histogram Oriented Gradients}\label{sec:hog}
%-------------------------------------------------%
Histogram oriented gradients (HOG) is a feature extraction method that
computes centered horizontal and vertical gradients with no smoothing.
Originally created for human detection, .... This computes the
gradient orientation and magnitudes in an image by first dividing the
image into $n\times n$ blocks of X\% overlap.  The gradient
orientation is then quantized into m bins. ``(optional) global image
normalisation computing the gradient image in x and y computing
gradient histograms normalising across blocks flattening into a
feature vector''\citep{dalalandtriggs_05}.

% Need to look into edge orientation histograms, scale-invariant
% feature transform descriptors, shape contexts.


% NEED TO FEED THE DESCRIPTORS TO SOMETHING WITH SUPERVISED LEARNING.
% (e.g. SVM, Neural network).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\begin{figure*}[t]\label{fig:hogvisual}
\begin{center}
\includegraphics[width=1\columnwidth]{figures/105_lensed_imgs_1or5ppc32cpb1.pdf}\hspace{-20pt}
\includegraphics[width=1\columnwidth]{figures/105_lensed_imgs_1or9ppc16cpb1.pdf}
\caption{Visualized HOG feature vectors of the sample lensed HST-like
  image from Figure~\ref{fig:mockimages}.  Left: The corresponding HOG
  visualization with N$_\text{orient}=5$, (32, 32) pixels per cell,
  and (1, 1) cells per block.  Right: The HOG visualization of the
  same mock image with N$_\text{orient}=9$, (16, 16) pixels per cell,
  and (1, 1) cells per block.  The brightest lines align with
  prominent edges.  Note, while fewer pixels per cell will better
  resolve the arc feature, the ``edge'' corresponding to the arc is
  less prominent.  }
\end{center}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            


%-------------------------------------------------%
\subsection{Machine Learning Algorithm: Logistic Regression}\label{sec:logreg}
%-------------------------------------------------%

Logisitic regression is a linear classifier, which finds the optimal
hyperplane between two classes of data points.  This hyperplane
defines the optimal model.  We must regularize the model to prevent
overfitting.  

In regularization, the goal is to minimize a cost function with
respect to ....  Larger values of $C_\text{LogReg}$ correspond to
increasing model complexity.  Figure~\ref{fig:regularization} shows
the accuracy of a model for selected HOG parameters as a function of
the regularization parameter, $C_\text{LogReg}$.  We plot the
regularization dependency for HOG parameterization of
$N_\text{orient}=9$, $PPC=(16, 16)$, $CPB=(1,1)$.  

The accuracy of a model can be defined as either the score of the
model on a test set, or the AUC for the ROC curve.


% Regularization coefficient:
% http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex5/ex5.html

%-------------------------------------------------%
\subsection{Machine Learning Algorithm: Support Vector Machine}
%-------------------------------------------------%


%-------------------------------------------------%
%\subsection{Deep Learning Algorithm}
%-------------------------------------------------%

%-------------------------------------------------%
\section{Results}
\label{sec:results}
%-------------------------------------------------%
%-------------------------------------------------%
\subsection{Optimized pipeline parameters}\label{sec:gridsearch}
%-------------------------------------------------%

\begin{table}
\caption{Grid Search of Pipeline Parameters}
\begin{center}
\begin{tabular}{cccccc}
\hline \\ [-0.2cm]
N$_{orient}$ & Pixels/Cell & Cells/Block & C$_{reg}$ & HOG size & Score \\ [0.2cm]
\hline \\ [-0.2cm]
\multicolumn{6}{c}{(a) HST-like data} \\ [0.2cm]
\hline \\ [-0.2cm]
9 & (4, 4) & (3, 3) & 50. & &$0.76181\pm0.00491$\\ [0.2cm]
9 & (4, 4) & (1, 1) & 50. & &$0.83403\pm0.00547$\\ [0.2cm]
9 & (8, 8) & (3, 3) & 50. & &$0.85417\pm0.00851$\\ [0.2cm] 
9 & (8, 8) & (1, 1) & 50. & &$0.86389\pm0.00687$\\ [0.2cm] 
9 & (16, 16) & (1, 1) & 50. & & $0.88542\pm0.00680$ \\ [0.2cm]
9 & (16, 16) & (3, 3) & 10. & &$0.89306\pm0.00687$ \\ [0.2cm]
8 & (16, 16) & (3, 3) & 50. & &$0.90000\pm0.00170$ \\ [0.2cm]
9 & (16, 16) & (3, 3) & 50. & &$0.90139\pm0.00804$ \\ [0.2cm]
\hline \\ [-0.2cm]
\multicolumn{6}{c}{(b) LSST-like data} \\ [0.2cm]
\hline \\ [-0.2cm]
& & & & &\\
& & & & &\\ [0.2cm]
\hline
\end{tabular}
\end{center}
\label{tab:gridsearch}
\tablecomments{Panel (a) shows the results of a grid search across a
  range of HOG parameters and regression coefficient with training and
  test sets of size 1440 with the HST-like mock dataset.  Panel (b)
  shows the corresponding results for training and test sets of size
  5760 with the LSST mock dataset.  We explore different HOG
  parameters in each dataset due to resolution and image size
  differences.}
\end{table}

We run a grid search across parameters that should reasonably sample
the arc edges in either the HST- or LSST-like mock observations.
Recall, the HST-like images are $300\times300$ pixels per image, while
the LSST-like mock observations are $45\times45$ pixels per image.  In
general, must run a grid search in order to best optimize the pipeline
parameters.

We must first estimate the size of a cell that will contain a coherent
arc feature.  To first order approximation, subdivisions of cells that
are 1/100th the area of the entire image should contain coherent arc
edges that span an elongated shape within arc-containing cells.
Therefore, we perturb the pixels per cell parameter about (30, 30) for
the HST-like images, and (4, 4) for the LSST-like images.

Next, the cells per block parameter determines the normalization of
each cell with respect to the neighboring cell.  In general, this will
downweight arc-like edges in cells that neighbor very bright cells,
such as cells that cover the BCG.  We therefore vary the cells per
block parameter between (1, 1) and (3, 3).

The number of orientations will determine the sampling of rounded
edges.  For example, if we only have two orientations, an arc-like
feature in a cell will appear as .... However, contributions from a
cluster or line-of-sight galaxy in the same cell will ... the
normalizations in the histogram.  The resolution of the overall image
will also limit the additional information that an increase in
$N_\text{orient}$ will provide.  For the LSST-like data,
$N_\text{orient}=9$ compared with $N_\text{orient}=5$ only
results in a $???\%$ improvement.  In the case of the HST-like data,
$N_\text{orient}=9$ compared with $N_\text{orient}=5$
results in a $???\%$ improvement.

Note, we also include the size of the extracted features, e.g. the
length of the HOG feature vector.  A longer feature vector will
increase the amount of time required to train the model.
Additionally, for fixed memory restrictions, there is a tradeoff
between the length of the feature vector and the size of the training
set.  The logistic regression coefficient will also increase the
training time, \todo{since larger coefficients correspond to a more
  singular matrix}.  We discuss the score dependence on logistic
regression coefficient in Section\ref{sec:unregularizedfit}.

%-------------------------------------------------%
\subsection{Receiver Operatic Characteristic}
%-------------------------------------------------%

In this section, we discuss the Receiver Operating Characteristic
(ROC) curve, which shows the true positive rate as a function of false
positive rate for a given model and test set.  The true positive rate
is defined as the number of true positives divided by the total number
of positives (completeness of the positive identifications).  The
false positive rate is defined as the number of false positives
divided by the total number of negatives (impurity of the
identifications).  The ROC curve illustrates the performance of our
trained model as we vary the discrimination threshold.

The classifier model assigns a score to each test image, which is a
probability that the image is a strong lensing system.  To construct
the ROC curve, the test images are ranked by probability and the
discrimination threshold varied.  

For very high discrimination threshold, we have maximal purity, but a
low completeness (bottom left region of the ROC).  FOr a very low
discrimination threshold, we have maximal completeness, but minimal
purity (top right region of the ROC).  The ideal model would have a
data point on the ROC curve that sits on the top left corner,
i.e. $(x, y) = (0, 1)$.

In the context of strong lensing systems, we wish to maximize the true
positive rate so we have a representative count of the fraction of
strong lensing systems in an observed volume of the universe.  We also
want to minimize the false positive rate.  Positively identified
strong lensing systems will require expensive spectroscopic follow-up
for validation.  The steepness of the ROC curve indicates how well the
model will optimize the two, and we can characterize the steepness by
the area under the curve (AUC).  A perfect model would have an AUC of
1, with data points along $(x=0, 0\leq y\leq1)$, and $(0\leq x\leq 1,
y=1)$.

Figure~\ref{fig:ROC} shows the ROC curves for three different HOG
parameterizations for the HST-like dataset in red, green, and blue.
The parameters and corresponding AUC for each curve are labeled in the
legend.  The models for the HST-like dataset perform well, and have
AUCs very close to 1.  On the other hand, the model for our LSST-like
dataset does not perform well.  The dashed diagonal like corresponds
to a model that is as good as a random classifier, e.g. a coin-flip
for each test image.  The LSST-like ROC curve does marginally better,
and the $AUC=0.527$ is within the statistical noise of the sample.

We train the model corresponding to each curve with the logistic
regression regularization parameter, $C_\text{LogReg}$, that best
describes the training dataset without overfitting.  We discuss the
choice of regularization parameter in
Section~\ref{sec:regularizedfit}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\begin{figure}[t]\label{fig:ROC}
\begin{center}
\includegraphics[width=\columnwidth]{figures/ROC_multipleHOG.png}
\caption{Red, green, blue: ROC curves for a models trained on the
  HST-like data with the Logistic Regression algorithm.  The magenta
  shows the ROC curve for the LSST-like data.  The corresponding
  parameters are labeled, N for number of orientations, ppc for
  pixels-per-cell, cpb for cells-per-block, and C for best
  regularization parameter.  }
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            

%-------------------------------------------------%
\subsection{Effects of Regularization on Model Performance}\label{sec:regularizedfit}
%-------------------------------------------------% 

% Logistic regression and regularization
As described in Section~\ref{sec:logreg}, logistic regression trains a
model with complexity determined by the regularization parameter,
$C_\text{LogReg}$.  Increasing model complexity (larger
$C_\text{LogReg}$), will better describe features in the training
set[wc?].  However, an overly complex model will overfit the training
set at the expense of its performance on any independent test set.
The regularization parameter ultimately defines the model performance,
and we must perform a parameter search to identify the optimal value
for $C_\text{LogReg}$.

Figure~\ref{fig:regularization} shows the model performance as a
function of regularization parameter.  The left panel shows the
accuracy, or score, of the model as a function of regularization
parameter.  The right panel shows the AUC of the model as a function
of regularization parameter.  The solid and dotted lines respectively
correspond to the model performance on the test and training set.

For either characterization of the model, the model performance on the
training set increases and asymptotes with $C_\text{LogReg}$.  With
increasing model complexity, the model better fits the data set it has
been trained on.  This is analogous to fitting a seventh order
polynomial to seven data points, where the fitting function will go
through every point.  

EDITING HERE


% Scores vs. regularization on larger train/test set with comparable
% grid search in parameterization
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        
\begin{figure*}[t]\label{fig:regularization}
\begin{center}
\includegraphics[width=\columnwidth]{figures/regularized_accuracy.pdf}\hspace{-20pt}
\includegraphics[width=1\columnwidth]{figures/regularized_AUC.pdf}
\caption{Left: Accuracy of the model with varying regularization.
  Each color corresponds to a model trained for features extracted
  with a different set of HOG parameters.  The dotted (solid) lines
  show the performance of the model on the training (test) set.
  Right: AUC of the model with varying regularization.  Linestyles and
  colors are similar to those in the left panel.  AUC is a better
  characterization of model performance, since the accuracy is
  determined for fixed classification threshold [wc?], but peak model
  performance can occur at a different classification threshold.}
\end{center}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            


%-------------------------------------------------%
\subsection{Failed identifications}\label{sec:failedids}
%-------------------------------------------------%

The best ROC curve, HST:N5ppc32cpb4C100, illustrates that we can lower
the classification threshold and improve the true positive rate by
$\sim10\%$ and sacrifice an increase in false positive rate by
$\lsim5\%$.  In this section, we examine the ``borderline'' images,
which transition from negative classification to positive calculation
in the steepest part of the AUC curve.  Borderline images correspond
to candidates for spectroscopic follow-up.  The model is less
confident about this subset of images, and these would therefore be
good candidates for follow-up human classification and/or
spectroscopic follow-up (CITE HERE???).

\todo{Figure ~\ref{fig:falseids} shows a random sample of 10 borderline
images.  Common features...????}


%-------------------------------------------------%
\subsection{Effects of BCG contamination}
%-------------------------------------------------%

Previous works to identify strong lensing systems have often modeled
out the BCG from images to better identify lensed images (???CITE
HERE???).  Here, we test the effects of the BCG by training and
testing models on mock data where the BCG has not been included.  By
construction, these results show the maximal improvement that can be
obtained from BCG subtraction.

Figure~\ref{fig:ROCnobcg} shows the \todo{ROC curve for models of HOG
  feature vectors of mock data with no BCG.  The feature vectors are
  the same HOG parameters as in Figure~\ref{fig:ROC} }.  \todo{The
  grid search over parameter space was similar to that for data with
  the BCG.}

???????

%-------------------------------------------------%
\section{Summary and Discussions}
\label{sec:conclusions}
%-------------------------------------------------%

We summarize key points below:

%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{itemize}

\item 
    
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%

These results indicate that...

\acknowledgments CA acknowledges support from the Kavli Institute of
Cosmological Physics, the Enrico Fermi Institute at the University of
Chicago, and the University of Chicago Provost's Office.
\lastpagefootnotes


\bibliography{ms}

\end{document}  

